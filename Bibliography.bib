@software{abadiTensorFlowLargescaleMachine2015,
  title = {{{TensorFlow}}, {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2015-11},
  doi = {10.5281/zenodo.4724125},
  url = {https://github.com/tensorflow/tensorflow},
  urldate = {2025-03-12},
  abstract = {An Open Source Machine Learning Framework for Everyone}
}

@online{alemohammadSelfConsumingGenerativeModels2023,
  title = {Self-{{Consuming Generative Models Go MAD}}},
  author = {Alemohammad, Sina and Casco-Rodriguez, Josue and Luzi, Lorenzo and Humayun, Ahmed Imtiaz and Babaei, Hossein and LeJeune, Daniel and Siahkoohi, Ali and Baraniuk, Richard G.},
  date = {2023-07-04},
  eprint = {2307.01850},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.01850},
  url = {http://arxiv.org/abs/2307.01850},
  urldate = {2025-03-19},
  abstract = {Seismic advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.},
  pubstate = {prepublished}
}

@article{aliTextbasedSteganographyUsing2021,
  title = {Text-Based {{Steganography}} Using {{Huffman Compression}} and {{AES Encryption Algorithm}}},
  author = {Ali, Rawaa Hamza and Kadhim, Jamal Mohamed},
  date = {2021-11-30},
  journaltitle = {Iraqi Journal of Science},
  pages = {4110--4120},
  issn = {2312-1637},
  doi = {10.24996/ijs.2021.62.11.31},
  url = {https://ijs.uobaghdad.edu.iq/index.php/eijs/article/view/3488},
  urldate = {2025-03-12},
  abstract = {In every system of security, to keep important data confidential, we need a high degree of protection. Steganography can be defined as a way of sending confidential texts through a secure medium of communications as well as protecting the information during the process of transmission. Steganography is a technology that is used to protect users' security and privacy. Communication is majorly achieved using a network through SMS, e-mail, and so on. The presented work suggested a technology of text hiding for protecting secret texts with Unicode characters. The similarities of glyphs\&nbsp; provided invisibility and increased the hiding capacity. In conclusion, the proposed method succeeded in securing confidential data and achieving high payload capacity by using the Huffman compression algorithm, which was implemented on an unlimited text length. In addition, this approach has the ability to hide a single bit in every digit or letter in the cover file. Also, the approach meets the cognitive transparency and does not make the modifications apparent on the original data. The method suggested in this work increases the security level through coding a secret message before embedding it within the cover text, with the use of the Advanced Encryption Standard (AES) algorithm.},
  langid = {english}
}

@inproceedings{allaEvolutionHindiText2009,
  title = {An {{Evolution}} of {{Hindi Text Steganography}}},
  booktitle = {2009 {{Sixth International Conference}} on {{Information Technology}}: {{New Generations}}},
  author = {Alla, Kalavathi and Prasad, R. Siva Rama},
  date = {2009-04},
  pages = {1577--1578},
  doi = {10.1109/ITNG.2009.41},
  url = {https://ieeexplore.ieee.org/abstract/document/5070855},
  urldate = {2025-03-24},
  abstract = {This paper presents a novel steganography scheme suitable for Hindi text. It can be classified under text steganography. Conveying information secretly and establishing a hidden relationship between the message and its counterpart has been of great interest since very long time ago. Methods of steganography are mostly applied on images, audio, video and text files. During the process characteristics of these methods are to change in the structure and features so as not to be identifiable by human eye. Text documents are the best examples for this. This paper presents a novel Hindi text steganography, which uses Hindi letters and its diacritics and numerical code. This method is not only useful to Hindi text but also to all other similar Indian languages.},
  eventtitle = {2009 {{Sixth International Conference}} on {{Information Technology}}: {{New Generations}}}
}

@article{andersonLimitsSteganography1998,
  title = {On the Limits of Steganography},
  author = {Anderson, R.J. and Petitcolas, F.A.P.},
  date = {1998-05},
  journaltitle = {IEEE Journal on Selected Areas in Communications},
  volume = {16},
  number = {4},
  pages = {474--481},
  issn = {1558-0008},
  doi = {10.1109/49.668971},
  url = {https://ieeexplore.ieee.org/abstract/document/668971},
  urldate = {2025-03-23},
  abstract = {In this paper, we clarify what steganography is and what it can do. We contrast it with the related disciplines of cryptography and traffic security, present a unified terminology agreed at the first international workshop on the subject, and outline a number of approaches-many of them developed to hide encrypted copyright marks or serial numbers in digital audio or video. We then present a number of attacks, some new, on such information hiding schemes. This leads to a discussion of the formidable obstacles that lie in the way of a general theory of information hiding systems (in the sense that Shannon gave us a general theory of secrecy systems). However, theoretical considerations lead to ideas of practical value, such as the use of parity checks to amplify covertness and provide public key steganography. Finally, we show that public key information hiding systems exist, and are not necessarily constrained to the case where the warden is passive.},
  eventtitle = {{{IEEE Journal}} on {{Selected Areas}} in {{Communications}}}
}

@software{anselPyTorch2Faster2024,
  title = {{{PyTorch}} 2: {{Faster Machine Learning Through Dynamic Python Bytecode Transformation}} and {{Graph Compilation}}},
  shorttitle = {{{PyTorch}} 2},
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and family=Luk, given=CK, given-i=CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  date = {2024-04},
  origdate = {2016-08-13T05:26:41Z},
  journaltitle = {29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)},
  doi = {10.1145/3620665.3640366},
  url = {https://pytorch.org/assets/pytorch2-2.pdf},
  urldate = {2025-03-12},
  abstract = {Tensors and Dynamic neural networks in Python with strong GPU acceleration},
  organization = {ACM}
}

@article{bagnallReversingSteganographyMyth2002,
  title = {Reversing the {{Steganography Myth}} in {{Terrorist Operations}}: {{The Asymmetrical Threat}} of {{Simple Intelligence Dissemination Techniques Using Common Tools}}},
  author = {Bagnall, Robert J.},
  date = {2002},
  journaltitle = {SANS Information Security Reading Room},
  volume = {19},
  url = {http://profs.sci.univr.it/~giaco/download/Watermarking-Obfuscation/paper556.pdf},
  abstract = {The events of September 11th prompted significant discussion and speculation as to the use of Steganography by terrorists for clandestine and secured communications. Numerous prominent figures in the industry have written articles and given interviews debating whether or not terrorists are using Stego to disseminate information to sleeper cells both in America and abroad. USA Today, for example, quoted “US Officials” this way: “U.S. officials and experts say it's the latest method of communication being used by Osama bin Laden and his associates to outfox law enforcement. Bin Laden and others are hiding maps and photographs of terrorist targets and posting instructions for terrorist activities on sports chat rooms, pornographic bulletin boards and other Web sites, U.S. and foreign officials say.” (http://www.usatoday.com/life/cyber/tech/2001-0205-binladen.htm) Mostly, the commentary was not a question of if but rather how long. I contend, however, that Steganography is not required, nor significantly used, by terrorist organizations for a number of reasons. Commonly available IT software and equipment such as 802.11b wireless networks, laptop and desktop computers, highcapacity media devices, and a little creative thinking, make it possible, indeed simple, to facilitate efficient, short-duration, and completely anonymous communications between even casual hosts. In this paper, using common technology, I will demonstrate various ways and methods for simple, clandestine communications that are virtually undetectable and untraceable. In order to be most effective, clandestine data transmission between parties must be simple, stealthy, and efficient. Many would say security of the data is important, but data security in this case can also be viewed as a vector of the exposure time of the data in question to outside parties. Additionally, focus will be given to both short and long range data transmission, including transmission through methods as simple as a physical hand-off of data between parties to more complicated means across larger distances between parties which do not have physical contact, such as wireless and Internet transmissions. First we will examine three high-capacity data storage devices, their immunity to detection, and the ease with which they can be transferred between parties. Next, we will examine short burst dissemination through the use of wireless transmissions in high-density populations, such as Washington, DC or San Francisco. Lastly, we will examine the use of the web in simple, effective, and virtually undetectable intelligence dissemination.},
  langid = {english}
}

@online{baraniukWhyPrintersAdd2017,
  title = {Why Printers Add Secret Tracking Dots},
  author = {Baraniuk, Chris},
  date = {2017-06-07},
  url = {https://www.bbc.com/future/article/20170607-why-printers-add-secret-tracking-dots},
  urldate = {2025-01-23},
  abstract = {They’re almost invisible but contain a hidden code – and now their presence on a leaked document has sparked speculation about their usefulness to FBI investigators.},
  langid = {british}
}

@article{bbcGhislaineMaxwellJeffreyEpstein2020,
  entrysubtype = {newspaper},
  title = {Ghislaine {{Maxwell-Jeffrey Epstein}} Emails Revealed in New Court Papers},
  author = {{BBC}},
  date = {2020-07-31},
  url = {https://www.bbc.com/news/world-us-canada-53605784},
  urldate = {2024-12-08},
  abstract = {In the papers, a key accuser also alleges the pair were equally involved in sex trafficking.},
  langid = {british}
}

@article{bbcPegasusSpywareSold2021,
  entrysubtype = {newspaper},
  title = {Pegasus: {{Spyware}} Sold to Governments 'Targets Activists'},
  shorttitle = {Pegasus},
  author = {{BBC}},
  date = {2021-07-18},
  url = {https://www.bbc.com/news/technology-57881364},
  urldate = {2024-12-08},
  abstract = {Israeli tech firm NSO denies media reports that its software has been sold to authoritarian regimes.},
  langid = {british}
}

@misc{bennettLinguisticSteganographySurvey2004,
  title = {Linguistic Steganography: {{Survey}}, Analysis, and Robustness Concerns for Hiding Information in Text},
  author = {Bennett, Krista},
  date = {2004-05-12},
  abstract = {Steganography is an ancient art. With the advent of computers, we have vast accessible bodies of data in which to hide information, and increasingly sophisticated techniques with which to analyze and recover that information. While much of the recent research in steganography has been centered on hiding data in images, many of the solutions that work for images are more complicated when applied to natural language text as a cover medium. Many approaches to steganalysis attempt to detect statistical anomalies in cover data which predict the presence of hidden information. Natural language cover texts must not only pass the statistical muster of automatic analysis, but also the minds of human readers. Linguistically na},
  acknowledgement = {Victor Raskin, CERIAS},
  affiliation = {Interdepartmental Program in Linguistics and CERIAS},
  contents = {- Steganography, steganalysis and mimicking - Text steganography - Linguistic concerns with existing methods - Future directions in constructing linguistically and statistically robust cover texts},
  howpublished = {Research paper accepted in partial fulfillment of the Dept. of Linguistics preliminary examination requirement},
  langid = {english},
  organization = {Purdue University},
  subject = {Issues and future directions in linguistic steganography}
}

@article{brittainMetaKnewIt2025,
  entrysubtype = {newspaper},
  title = {Meta Knew It Used Pirated Books to Train {{AI}}, Authors Say},
  author = {Brittain, Blake},
  date = {2025-01-09T21:58:00Z},
  journaltitle = {Reuters},
  url = {https://www.reuters.com/technology/artificial-intelligence/meta-knew-it-used-pirated-books-train-ai-authors-say-2025-01-09/},
  urldate = {2025-03-19},
  abstract = {Meta Platforms used pirated versions of copyrighted books to train its artificial intelligence systems with approval from its CEO Mark Zuckerberg, a group of authors alleged in newly disclosed court papers.},
  journalsubtitle = {Artificial Intelligence},
  langid = {english}
}

@online{carreiraRevolutionizingMobileInteraction2023,
  title = {Revolutionizing {{Mobile Interaction}}: {{Enabling}} a 3 {{Billion Parameter GPT LLM}} on {{Mobile}}},
  shorttitle = {Revolutionizing {{Mobile Interaction}}},
  author = {Carreira, Samuel and Marques, Tomás and Ribeiro, José and Grilo, Carlos},
  date = {2023-09-29},
  eprint = {2310.01434},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2310.01434},
  url = {http://arxiv.org/abs/2310.01434},
  urldate = {2024-11-12},
  abstract = {The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, and future directions of on-device LLM inference. This breakthrough technology opens up possibilities for empowering users with sophisticated AI capabilities while preserving their privacy and eliminating latency concerns.},
  pubstate = {prepublished},
  version = {1}
}

@online{chenLLMMobileInitial2024,
  title = {{{LLM}} for {{Mobile}}: {{An Initial Roadmap}}},
  shorttitle = {{{LLM}} for {{Mobile}}},
  author = {Chen, Daihang and Liu, Yonghui and Zhou, Mingyi and Zhao, Yanjie and Wang, Haoyu and Wang, Shuai and Chen, Xiao and Bissyandé, Tegawendé F. and Klein, Jacques and Li, Li},
  date = {2024-07-09},
  eprint = {2407.06573},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2407.06573},
  url = {http://arxiv.org/abs/2407.06573},
  urldate = {2024-11-12},
  abstract = {When mobile meets LLMs, mobile app users deserve to have more intelligent usage experiences. For this to happen, we argue that there is a strong need to appl LLMs for the mobile ecosystem. We therefore provide a research roadmap for guiding our fellow researchers to achieve that as a whole. In this roadmap, we sum up six directions that we believe are urgently required for research to enable native intelligence in mobile devices. In each direction, we further summarize the current research progress and the gaps that still need to be filled by our fellow researchers.},
  pubstate = {prepublished},
  version = {1}
}

@online{chenOptimizationArmv9Architecture2024,
  title = {Optimization of {{Armv9}} Architecture General Large Language Model Inference Performance Based on {{Llama}}.Cpp},
  author = {Chen, Longhao and Zhao, Yina and Xie, Qiangjun and Sheng, Qinghua},
  date = {2024-06-16},
  eprint = {2406.10816},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.10816},
  url = {http://arxiv.org/abs/2406.10816},
  urldate = {2024-12-05},
  abstract = {This article optimizes the inference performance of the Qwen-1.8B model by performing Int8 quantization, vectorizing some operators in llama.cpp, and modifying the compilation script to improve the compiler optimization level. On the Yitian 710 experimental platform, the prefill performance is increased by 1.6 times, the decoding performance is increased by 24 times, the memory usage is reduced to 1/5 of the original, and the accuracy loss is almost negligible.},
  pubstate = {prepublished}
}

@inproceedings{christUndetectableWatermarksLanguage2024,
  title = {Undetectable {{Watermarks}} for {{Language Models}}},
  booktitle = {Proceedings of {{Thirty Seventh Conference}} on {{Learning Theory}}},
  author = {Christ, Miranda and Gunn, Sam and Zamir, Or},
  date = {2024-06-30},
  pages = {1125--1139},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v247/christ24a.html},
  urldate = {2025-03-12},
  abstract = {Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text.  Prior works have suggested methods of embedding watermarks in model outputs, by *noticeably* altering the output distribution. We ask: Is it possible to introduce a watermark without incurring *any detectable* change to the output distribution? To this end, we introduce a cryptographically-inspired notion of undetectable watermarks for language models.  That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.},
  eventtitle = {The {{Thirty Seventh Annual Conference}} on {{Learning Theory}}},
  langid = {english}
}

@online{cohenWhenTerrorHides2001,
  title = {When {{Terror Hides Online}}},
  author = {Cohen, Adam},
  date = {2001-11-12T05:00:00},
  url = {https://time.com/archive/6665170/when-terror-hides-online/},
  urldate = {2025-01-09},
  abstract = {Did you hear the one about Osama bin Laden hiding messages in porn websites? It sounds like one of those crazy Sept. 11 rumors, but it's actually a law-enforcement theory about how the al-Qaeda...},
  langid = {english},
  organization = {TIME}
}

@article{conwayCodeWarsSteganography2003,
  title = {Code {{Wars}}: {{Steganography}}, {{Signals Intelligence}}, and {{Terrorism}}},
  author = {Conway, Maura},
  date = {2003-02-16},
  url = {https://doras.dcu.ie/494/1/know_tech_pol_16_2_2003.pdf},
  abstract = {This paper describes and discusses the process of secret communication known as steganography. The argument advanced here is that terrorists are unlikely to be employing digital steganography to facilitate secret intra-group communication as has been claimed. This is because terrorist use of digital steganography is both technically and operationally implausible. The position adopted in this paper is that terrorists are likely to employ low-tech steganography such as semagrams and null ciphers instead.},
  langid = {english}
}

@online{dasSteganographySteganalysisDifferent2011,
  title = {Steganography and {{Steganalysis}}: {{Different Approaches}}},
  shorttitle = {Steganography and {{Steganalysis}}},
  author = {Das, Soumyendu and Das, Subhendu and Bandyopadhyay, Bijoy and Sanyal, Sugata},
  date = {2011-11-16},
  eprint = {1111.3758},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1111.3758},
  url = {http://arxiv.org/abs/1111.3758},
  urldate = {2025-03-12},
  abstract = {Steganography is the technique of hiding confidential information within any media. Steganography is often confused with cryptography because the two are similar in the way that they both are used to protect confidential information. The difference between the two is in the appearance in the processed output; the output of steganography operation is not apparently visible but in cryptography the output is scrambled so that it can draw attention. Steganlysis is process to detect of presence of steganography. In this article we have tried to elucidate the different approaches towards implementation of steganography using 'multimedia' file (text, static image, audio and video) and Network IP datagram as cover. Also some methods of steganalysis will be discussed.},
  pubstate = {prepublished},
  version = {1}
}

@article{dembartEndUserHide2001,
  entrysubtype = {newspaper},
  title = {The {{End User}}: {{Hide Your Secrets}}},
  shorttitle = {The {{End User}}},
  author = {Dembart, Lee},
  date = {2001-05-07},
  journaltitle = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2001/05/07/business/worldbusiness/IHT-the-end-user-hide-your-secrets.html},
  urldate = {2025-03-11},
  abstract = {Everyone knows by now that the Internet is insecure — the bad guys can eavesdrop on what you send over the Web — and most people probably know that encryption is one way around the problem. If there's something that you don't want anyone but},
  journalsubtitle = {Business},
  langid = {american}
}

@software{devitoZdevitoATen2025,
  title = {Zdevito/{{ATen}}},
  author = {DeVito, Zachary},
  date = {2025-03-02T13:28:59Z},
  origdate = {2017-06-20T23:50:41Z},
  url = {https://github.com/zdevito/ATen},
  urldate = {2025-03-11},
  abstract = {ATen: A TENsor library for C++11}
}

@online{donnerFinetuningLLMYour2024,
  title = {Fine-Tuning an {{LLM}} on Your Texts: Part 2 - Exploring Your Text Data},
  shorttitle = {Fine-Tuning an {{LLM}} on Your Texts},
  author = {Donner, Edward},
  date = {2024-01-17T15:10:39+00:00},
  url = {https://edwarddonner.com/2024/01/17/fine-tune-llm-on-texts-part-2-the-data/},
  urldate = {2025-01-24},
  abstract = {In part 2 of my guide to fine-tuning an LLM on your text messages, we use our CSV downloads to read, organize and investigate our data.},
  langid = {american},
  organization = {Edward Donner}
}

@online{donnerFinetuningLLMYour2024a,
  title = {Fine-Tuning an {{LLM}} on Your Texts: Part 3 - Curating the Dataset},
  shorttitle = {Fine-Tuning an {{LLM}} on Your Texts},
  author = {Donner, Edward},
  date = {2024-01-24T23:17:43+00:00},
  url = {https://edwarddonner.com/2024/01/24/fine-tuning-an-llm-on-your-texts-part-3-curating-the-dataset/},
  urldate = {2025-01-24},
  abstract = {In part 3 of my guide to fine-tuning an LLM on your text messages, we curate the dataset, encrypt and upload to Hugging Face.},
  langid = {american},
  organization = {Edward Donner}
}

@online{donnerFinetuningLLMYour2024b,
  title = {Fine-Tuning an {{LLM}} on Your Texts: Part 4 - {{QLoRA}}},
  shorttitle = {Fine-Tuning an {{LLM}} on Your Texts},
  author = {Donner, Edward},
  date = {2024-01-31T17:42:51+00:00},
  url = {https://edwarddonner.com/2024/01/31/fine-tuning-an-llm-on-your-text-messages-using-qlora/},
  urldate = {2025-01-24},
  abstract = {In part 4 of my guide to fine-tuning an LLM on your text message history, we train using QLoRA and experiment with hyper-parameters.},
  langid = {american},
  organization = {Edward Donner}
}

@online{donnerSimulationMeFinetuning2024,
  title = {A Simulation of Me: Fine-Tuning an {{LLM}} on 240k Text Messages},
  shorttitle = {A Simulation of Me},
  author = {Donner, Edward},
  date = {2024-01-02T21:26:11+00:00},
  url = {https://edwarddonner.com/2024/01/02/fine-tuning-an-llm-on-240k-text-messages/},
  urldate = {2025-01-24},
  abstract = {Armed with a download of my 240,000 text message and Whatsapp history, I was able to fine-tune an LLM to create convincing conversations.},
  langid = {american},
  organization = {Edward Donner}
}

@online{donnerStepStepGuide2024,
  title = {Step by Step Guide: Fine-Tune an {{LLM}} on Your Texts (Part 1)},
  shorttitle = {Step by Step Guide},
  author = {Donner, Edward},
  date = {2024-01-11T14:22:20+00:00},
  url = {https://edwarddonner.com/2024/01/11/fine-tune-llama-for-text-messages-part-1/},
  urldate = {2025-01-24},
  abstract = {Part 1 of my step-by-step guide on how to fine-tune an LLM on your entire text message and WhatsApp history.},
  langid = {american},
  organization = {Edward Donner}
}

@software{drukAndriydrukLMPlayground2025,
  title = {Andriydruk/{{LMPlayground}}},
  author = {Druk, Andriy},
  date = {2025-03-01T12:44:43Z},
  origdate = {2024-03-16T13:51:07Z},
  url = {https://github.com/andriydruk/LMPlayground},
  urldate = {2025-03-11},
  abstract = {Language Model Playground}
}

@article{edwardsFBIPaidMore2016,
  entrysubtype = {newspaper},
  title = {{{FBI}} Paid More than \$1.3 Million to Break into {{San Bernardino iPhone}}},
  author = {Edwards, Julia},
  date = {2016-04-22T17:52:08},
  journaltitle = {Reuters},
  url = {https://www.reuters.com/article/technology/fbi-paid-more-than-13-million-to-break-into-san-bernardino-iphone-idUSKCN0XI2IB/},
  urldate = {2025-02-18},
  abstract = {Federal Bureau of Investigation Director James Comey said on Thursday the agency paid more to get into the iPhone of one of the San Bernardino shooters than he will make in the remaining seven years and four months he has in his job.},
  journalsubtitle = {Technology},
  langid = {american}
}

@online{eldanTinyStoriesHowSmall2023,
  title = {{{TinyStories}}: {{How Small Can Language Models Be}} and {{Still Speak Coherent English}}?},
  shorttitle = {{{TinyStories}}},
  author = {Eldan, Ronen and Li, Yuanzhi},
  date = {2023-05-24},
  eprint = {2305.07759},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2305.07759},
  url = {http://arxiv.org/abs/2305.07759},
  urldate = {2024-11-27},
  abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
  pubstate = {prepublished}
}

@online{fangGeneratingSteganographicText2017,
  title = {Generating {{Steganographic Text}} with {{LSTMs}}},
  author = {Fang, Tina and Jaggi, Martin and Argyraki, Katerina},
  date = {2017-05-30},
  eprint = {1705.10742},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.10742},
  url = {http://arxiv.org/abs/1705.10742},
  urldate = {2025-01-26},
  abstract = {Motivated by concerns for user privacy, we design a steganographic system ("stegosystem") that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.},
  pubstate = {prepublished}
}

@online{fbiUpdateFBIInvestigation2024,
  type = {Press Release},
  title = {Update on the {{FBI Investigation}} of the {{Attempted Assassination}} of {{Former President Donald Trump}}},
  author = {{FBI}},
  date = {2024-07-15T15:03:00Z},
  url = {https://www.fbi.gov/news/press-releases/update-on-the-fbi-investigation-of-the-attempted-assassination-of-former-president-donald-trump},
  urldate = {2024-11-26},
  abstract = {The FBI continues to investigate the shooting incident at the July 13 rally in Butler, Pennsylvania, as an assassination attempt on former President Donald Trump and as potential domestic terrorism. The investigation is still in the early stages, and the FBI is providing the following updates.},
  langid = {american},
  organization = {Federal Bureau of Investigation}
}

@online{gaureL^2M^2C^22024,
  title = {L\textasciicircum 2 * {{M}}\textasciicircum 2 = {{C}}\textasciicircum 2: {{Large Language Models}} Are {{Covert Channels}}},
  author = {Gaure, Simen and Koffas, Stefanos and Picek, Stjepan and Rønjom, Sondre},
  date = {2024-10-07},
  eprint = {2405.15652},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.15652},
  url = {http://arxiv.org/abs/2405.15652},
  urldate = {2025-03-12},
  abstract = {Large Language Models (LLMs) have gained significant popularity recently. LLMs are susceptible to various attacks but can also improve the security of diverse systems. However, besides enabling more secure systems, how well do open source LLMs behave as covertext distributions to, e.g., facilitate censorship-resistant communication? In this paper, we explore open-source LLM-based covert channels. We empirically measure the security vs. capacity of an open-source LLM model (Llama-7B) to assess its performance as a covert channel. Although our results indicate that such channels are not likely to achieve high practical bitrates, we also show that the chance for an adversary to detect covert communication is low. To ensure our results can be used with the least effort as a general reference, we employ a conceptually simple and concise scheme and only assume public models.},
  pubstate = {prepublished},
  version = {2}
}

@software{gerganovGgerganovGgml2024,
  title = {Ggerganov/Ggml},
  author = {Gerganov, Georgi},
  date = {2024-12-08T06:39:36Z},
  origdate = {2022-09-18T17:07:19Z},
  url = {https://github.com/ggerganov/ggml},
  urldate = {2024-12-08},
  abstract = {Tensor library for machine learning}
}

@software{gerganovGgerganovLlamacpp2024,
  title = {Ggerganov/Llama.Cpp},
  author = {Gerganov, Georgi},
  date = {2024-12-08T12:18:03Z},
  origdate = {2023-03-10T18:58:00Z},
  url = {https://github.com/ggerganov/llama.cpp},
  urldate = {2024-12-08},
  abstract = {LLM inference in C/C++}
}

@software{gerganovGgerganovWhispercpp2024,
  title = {Ggerganov/Whisper.Cpp},
  author = {Gerganov, Georgi},
  date = {2024-12-08T15:39:54Z},
  origdate = {2022-09-25T18:26:37Z},
  url = {https://github.com/ggerganov/whisper.cpp},
  urldate = {2024-12-08},
  abstract = {Port of OpenAI's Whisper model in C/C++}
}

@software{ghorbaniAghorbaniPocketpalai2025,
  title = {A-Ghorbani/Pocketpal-Ai},
  author = {Ghorbani, Asghar},
  date = {2025-03-11T17:43:15Z},
  origdate = {2024-08-25T08:14:11Z},
  url = {https://github.com/a-ghorbani/pocketpal-ai},
  urldate = {2025-03-11},
  abstract = {An app that brings language models directly to your phone.}
}

@online{googleaiedgeteamTensorFlowLiteNow2024,
  title = {{{TensorFlow Lite}} Is Now {{LiteRT- Google Developers Blog}}},
  author = {{Google AI Edge team}},
  date = {2024-09-04},
  url = {https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/},
  urldate = {2025-03-18},
  abstract = {TensorFlow Lite, now named LiteRT, is still the same high-performance runtime for on-device AI, but with an expanded vision to support models authored in PyTorch, JAX, and Keras.},
  langid = {english}
}

@software{googleGoogleaiedgeLiteRT2025,
  title = {Google-Ai-Edge/{{LiteRT}}},
  author = {{Google}},
  date = {2025-03-18T20:52:11Z},
  origdate = {2024-09-04T03:33:35Z},
  url = {https://github.com/google-ai-edge/LiteRT},
  urldate = {2025-03-18},
  abstract = {LiteRT is the new name for TensorFlow Lite (TFLite). While the name is new, it's still the same trusted, high-performance runtime for on-device AI, now with an expanded vision.},
  organization = {google-ai-edge}
}

@software{googleGoogleSentencepiece2024,
  title = {Google/Sentencepiece},
  author = {{Google}},
  date = {2024-12-28T19:47:17Z},
  origdate = {2017-03-07T10:03:48Z},
  url = {https://github.com/google/sentencepiece},
  urldate = {2024-12-28},
  abstract = {Unsupervised text tokenizer for Neural Network-based text generation.},
  organization = {Google}
}

@article{hamzahLinguisticSteganographyFramework2021,
  title = {A Linguistic Steganography Framework Using {{Arabic}} Calligraphy},
  author = {family=Hamzah, given=Ali. A., given-i={{Ali}}A and Khattab, Sherif and Bayomi, Hanaa},
  date = {2021-09-01},
  journaltitle = {Journal of King Saud University - Computer and Information Sciences},
  shortjournal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {33},
  number = {7},
  pages = {865--877},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2019.04.015},
  url = {https://www.sciencedirect.com/science/article/pii/S1319157819301818},
  urldate = {2025-03-13},
  abstract = {In linguistic steganography, languages’ features are employed to hide information. The Arabic language has a rich set of features which have not been utilized until now in this area. In particular, Arabic calligraphy contains multiple fonts and multiple shapes of Arabic alphabet letters. In this paper, a framework that uses Arabic calligraphy to hide information is proposed. The phases of the framework are preparation phase, embedding phase and extraction phase. The embedding phase uses string matching to generate stego text and accompanying letter shapes according to a secret message. The framework also includes corpus creation and a modification of the Aho-Corasick string-matching algorithm. The Arabic font Naskh was used as a case study. A set of Arabic poetry and proverbs were used as a dataset. The framework was evaluated on capacity and security. Because the visual difference between the cover and the stego-cover must be unnoticeable to the human in any stego-system, the security in this framework is satisfying due there is no cover used. The cover represents the secret message itself and it provides high capacity to hide data also. The evaluation showed the potential of using the multiple shapes of Arabic letters to satisfy steganography requirements.}
}

@article{hanFolkloreSourceCoding2005,
  title = {Folklore in Source Coding: Information-Spectrum Approach},
  shorttitle = {Folklore in Source Coding},
  author = {Han, Te Sun},
  date = {2005-02},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {51},
  number = {2},
  pages = {747--753},
  issn = {1557-9654},
  doi = {10.1109/TIT.2004.840860},
  url = {https://ieeexplore.ieee.org/document/1386546},
  urldate = {2025-03-13},
  abstract = {Information theory has several traditional folklore problems about data compression or channel coding with reference to random number generation problems. Here, we focus on and reasonably formulate one of them from the viewpoint of information spectra. Specifically, we verify the validity of the folklore that the output from any source encoder working at the optimal coding rate with asymptotically vanishing probability of error looks like almost completely random.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}}
}

@article{haoReversibleNaturalLanguage2018,
  title = {Reversible Natural Language Watermarking Using Synonym Substitution and Arithmetic Coding},
  author = {Hao, Wei and Xiang, Lingyun and Li, Yan and Yang, Peng and Shen, Xiaobo},
  date = {2018},
  issn = {1546-2218},
  doi = {10.3970/cmc.2018.03510},
  url = {https://dr.ntu.edu.sg/handle/10356/106752},
  urldate = {2025-03-24},
  abstract = {For protecting the copyright of a text and recovering its original content harmlessly, this paper proposes a novel reversible natural language watermarking method that combines arithmetic coding and synonym substitution operations. By analyzing relative frequencies of synonymous words, synonyms employed for carrying payload are quantized into an unbalanced and redundant binary sequence. The quantized binary sequence is compressed by adaptive binary arithmetic coding losslessly to provide a spare for accommodating additional data. Then, the compressed data appended with the watermark are embedded into the cover text via synonym substitutions in an invertible manner. On the receiver side, the watermark and compressed data can be extracted by decoding the values of synonyms in the watermarked text, as a result of which the original context can be perfectly recovered by decompressing the extracted compressed data and substituting the replaced synonyms with their original synonyms. Experimental results demonstrate that the proposed method can extract the watermark successfully and achieve a lossless recovery of the original text. Additionally, it achieves a high embedding capacity.},
  langid = {english},
  annotation = {Accepted: 2019-06-26T05:16:58Z}
}

@online{hossainLLMProSAnalyzingLarge2025,
  title = {{{LLM-ProS}}: {{Analyzing Large Language Models}}' {{Performance}} in {{Competitive Problem Solving}}},
  shorttitle = {{{LLM-ProS}}},
  author = {Hossain, Md Sifat and Tabassum, Anika and Arefin, Md Fahim and Zaman, Tarannum Shaila},
  date = {2025-02-04},
  eprint = {2502.04355},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.04355},
  url = {http://arxiv.org/abs/2502.04355},
  urldate = {2025-03-24},
  abstract = {The rapid advancement of large language models has opened new avenues for automating complex problem-solving tasks such as algorithmic coding and competitive programming. This paper introduces a novel evaluation technique, LLM-ProS, to assess the performance of state-of-the-art LLMs on International Collegiate Programming Contest (ICPC) problems. Using a curated dataset of 166 World Finals problems from 2011 to 2024, we benchmark the models' reasoning, accuracy, and efficiency. We evaluate the five models-GPT-4o, Mistral Large, Llama-3.1-405B, and the o1 family, consisting of o1-mini and o1-preview, across critical metrics like correctness, resource utilization, and response calibration. Our results reveal significant differences in the models' abilities to generalize, adapt, and solve novel problems. We also investigated the impact of training methodologies, dataset contamination, and chain-of-thought reasoning on model performance. The findings provide new insights into optimizing LLMs for algorithmic tasks, highlighting both strengths and limitations of current models.},
  pubstate = {prepublished}
}

@article{huffmanMethodConstructionMinimumRedundancy1952,
  title = {A {{Method}} for the {{Construction}} of {{Minimum-Redundancy Codes}}},
  author = {Huffman, David A.},
  date = {1952-09},
  journaltitle = {Proceedings of the IRE},
  volume = {40},
  number = {9},
  pages = {1098--1101},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1952.273898},
  url = {https://ieeexplore.ieee.org/document/4051119},
  urldate = {2025-03-19},
  abstract = {An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.},
  eventtitle = {Proceedings of the {{IRE}}}
}

@online{huggingfaceModelsHuggingFace2025,
  title = {Models - {{Hugging Face}}},
  author = {{HuggingFace}},
  date = {2025-03-16},
  url = {https://huggingface.co/models},
  urldate = {2025-03-17},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.}
}

@software{jaxJaxmlJax2025,
  title = {Jax-Ml/Jax},
  author = {{Jax}},
  date = {2025-03-17T10:56:50Z},
  origdate = {2018-10-25T21:25:02Z},
  url = {https://github.com/jax-ml/jax},
  urldate = {2025-03-17},
  abstract = {Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more},
  organization = {jax-ml}
}

@article{kasterPrivatizedEspionageNSO2023,
  title = {Privatized Espionage: {{NSO Group Technologies}} and Its {{Pegasus}} Spyware},
  shorttitle = {Privatized Espionage},
  author = {Kaster, Sean D. and Ensign, Prescott C.},
  date = {2023},
  journaltitle = {Thunderbird International Business Review},
  volume = {65},
  number = {3},
  pages = {355--364},
  issn = {1520-6874},
  doi = {10.1002/tie.22321},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/tie.22321},
  urldate = {2025-03-12},
  abstract = {Advanced cyber technology like NSO Group Technologies' (NSO) controversial Pegasus spyware blurs distinctions between “good” and “bad.” This case follows the Israeli-based international leader in cyber espionage and developer NSO and one of its co-founders, Shalev Hulio from its creation in 2010 to the present. It includes NSO's acquisition by US-based private equity fund Francisco Partners in 2014. NSO's re-acquisition in 2019 by co-founders Hulio and Omri Lavie with funding support from London-based private equity fund Novalpina Capital. During this time, Pegasus had helped capture Mexican drug baron El Chapo, prevented terrorist attacks and broken up pedophilia, sex, and drug-trafficking rings. But Pegasus also contributed to the murder of Washington Post reporter Jamal Khashoggi as well as other illegal incidents against dissidents, journalist, and governments. As the case suggests, controlling access to such powerful technology that involves accountability, responsibility, and enforceability within a firm and within nations appears illusive.},
  langid = {english}
}

@online{kleinmanApplePullsData2025,
  title = {Apple Pulls Data Protection Tool after {{UK}} Government Security Row},
  author = {Kleinman, Zoe},
  date = {2025-02-22},
  url = {https://www.bbc.com/news/articles/cgj54eq4vejo},
  urldate = {2025-02-25},
  abstract = {Customers' photos and documents stored online will no longer be protected by end-to-end encryption.},
  langid = {british}
}

@online{kleinmanUKGovernmentDemands2025,
  title = {{{UK}} Government Demands Access to {{Apple}} Users' Encrypted Data},
  author = {Kleinman, Zoe},
  date = {2025-02-07},
  url = {https://www.bbc.com/news/articles/c20g288yldko},
  urldate = {2025-02-25},
  abstract = {The Home Office served the notice to the tech giant under the Investigatory Powers Act.},
  langid = {british}
}

@inproceedings{knochelTextSteganographyMethods2024,
  title = {Text {{Steganography Methods}} and Their {{Influence}} in {{Malware}}: {{A Comprehensive Overview}} and {{Evaluation}}},
  shorttitle = {Text {{Steganography Methods}} and Their {{Influence}} in {{Malware}}},
  booktitle = {Proceedings of the 2024 {{ACM Workshop}} on {{Information Hiding}} and {{Multimedia Security}}},
  author = {Knöchel, Mandy and Karius, Sebastian},
  date = {2024-06-24},
  series = {{{IH}}\&amp;{{MMSec}} '24},
  pages = {113--124},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3658664.3659637},
  url = {https://dl.acm.org/doi/10.1145/3658664.3659637},
  urldate = {2025-03-12},
  abstract = {Steganography describes techniques and algorithms for hiding secret information in a cover medium such as images, audio or text files. Malware that makes use of steganographic techniques, known as stegomalware, is becoming increasingly common. This paper provides a comprehensive analysis of various text steganography methods and their application in the context of stegomalware. We give an extensive overview of occurrences of text stegomalware in the real world and the steganographic methods used in these attacks. The cover text includes any files or data containing natural language text or machine-readable digital texts and source code such as HTML, CSS, JavaScript, etc. A categorical overview of known text steganography methods is presented, whereas text steganography techniques are classified into the categories insertion, substitution, permutation and generation. For each category, selected representatives have been practically implemented and tested with different cover text files and messages of varying lengths. The authors also look at real-world applications and instances of stegomalware that utilize these methods. The paper reveals that while there is a vast array of text steganography methods, only a few are used in practice. To assess the strengths and weaknesses of each method, the evaluation is based on the metrics capacity, imperceptibility and robustness, which are commonly used to evaluate steganographic methods, and additionally complexity. The evaluation results show the performance of each method based on the defined metrics. We further discuss possible countermeasures and their effect on each steganography method. The analysis also shows that with the rise of machine learning and large language models, text steganography methods might become more common in the future.},
  isbn = {979-8-4007-0637-0}
}

@article{kolataVeiledMessagesTerror2001,
  entrysubtype = {newspaper},
  title = {Veiled {{Messages}} of {{Terror May Lurk}} in {{Cyberspace}}},
  author = {Kolata, Gina},
  date = {2001-10-30},
  journaltitle = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2001/10/30/science/veiled-messages-of-terror-may-lurk-in-cyberspace.html},
  urldate = {2025-03-11},
  abstract = {Probe into terrorist attacks on United States is drawing new attention to steganography, stealthy way of sending messages through Internet by hiding them in digital photographs or music files; method allegedly was used by recently seized terrorists who were planning to blow up United States's Paris embassy; takes advantage of fact that digital files can be slightly altered and still look or sound same; computer programs that look for statistical deviations are reportedly detecting widespread use of steganography on Internet; diagram; photos (M)},
  journalsubtitle = {Science},
  langid = {american}
}

@article{kollnigAreIPhonesReally2022,
  title = {Are {{iPhones Really Better}} for {{Privacy}}? {{Comparative Study}} of {{iOS}} and {{Android Apps}}},
  shorttitle = {Are {{iPhones Really Better}} for {{Privacy}}?},
  author = {Kollnig, Konrad and Shuba, Anastasia and Binns, Reuben and Kleek, Max Van and Shadbolt, Nigel},
  date = {2022-04-01},
  journaltitle = {Proceedings on Privacy Enhancing Technologies},
  volume = {2022},
  number = {2},
  eprint = {2109.13722},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {6--24},
  issn = {2299-0984},
  doi = {10.2478/popets-2022-0033},
  url = {http://arxiv.org/abs/2109.13722},
  urldate = {2024-12-13},
  abstract = {While many studies have looked at privacy properties of the Android and Google Play app ecosystem, comparatively much less is known about iOS and the Apple App Store, the most widely used ecosystem in the US. At the same time, there is increasing competition around privacy between these smartphone operating system providers. In this paper, we present a study of 24k Android and iOS apps from 2020 along several dimensions relating to user privacy. We find that third-party tracking and the sharing of unique user identifiers was widespread in apps from both ecosystems, even in apps aimed at children. In the children's category, iOS apps tended to use fewer advertising-related tracking than their Android counterparts, but could more often access children's location. Across all studied apps, our study highlights widespread potential violations of US, EU and UK privacy law, including 1) the use of third-party tracking without user consent, 2) the lack of parental consent before sharing personally identifiable information (PII) with third-parties in children's apps, 3) the non-data-minimising configuration of tracking libraries, 4) the sending of personal data to countries without an adequate level of data protection, and 5) the continued absence of transparency around tracking, partly due to design decisions by Apple and Google. Overall, we find that neither platform is clearly better than the other for privacy across the dimensions we studied.}
}

@inproceedings{leeGitHubRecentBugs2024,
  title = {The {{GitHub Recent Bugs Dataset}} for {{Evaluating LLM-Based Debugging Applications}}},
  booktitle = {2024 {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Lee, Jae Yong and Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
  date = {2024-05},
  pages = {442--444},
  issn = {2159-4848},
  doi = {10.1109/ICST60714.2024.00049},
  url = {https://ieeexplore.ieee.org/abstract/document/10638568},
  urldate = {2025-03-24},
  abstract = {While Large Language Models (LLMs) have demon-strated strong natural language and code processing capabilities, concern has been raised as to whether existing bug benchmarks are included in their training data. We examine the training data of the open-source LLM StarCoder, and find it likely that data from the widely used Defects4J benchmark was included, raising the possibility of its inclusion in the training data of the GPT model as well. This makes it difficult to tell how well LLM-based results on Defects4J would generalize, as for any results it would be unclear whether a technique's performance is due to LLM generalization or memorization. To remedy this issue and facilitate continued research on LLM-based SE, we present the GitHub Recent Bugs (GHRB) framework, which continuously gathers real-world Java bugs for use in evaluation of LLM-based techniques. To date, we have gathered 89 bugs reported after the GPT-3.5 training data cutoff point of September 2021.},
  eventtitle = {2024 {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})}
}

@online{leeUnifiedDebuggingApproach2024,
  title = {A {{Unified Debugging Approach}} via {{LLM-Based Multi-Agent Synergy}}},
  author = {Lee, Cheryl and Xia, Chunqiu Steven and Yang, Longji and Huang, Jen-tse and Zhu, Zhouruixin and Zhang, Lingming and Lyu, Michael R.},
  date = {2024-10-23},
  eprint = {2404.17153},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.17153},
  url = {http://arxiv.org/abs/2404.17153},
  urldate = {2025-03-24},
  abstract = {Software debugging is a time-consuming endeavor involving a series of steps, such as fault localization and patch generation, each requiring thorough analysis and a deep understanding of the underlying logic. While large language models (LLMs) demonstrate promising potential in coding tasks, their performance in debugging remains limited. Current LLM-based methods often focus on isolated steps and struggle with complex bugs. In this paper, we propose the first end-to-end framework, FixAgent, for unified debugging through multi-agent synergy. It mimics the entire cognitive processes of developers, with each agent specialized as a particular component of this process rather than mirroring the actions of an independent expert as in previous multi-agent systems. Agents are coordinated through a three-level design, following a cognitive model of debugging, allowing adaptive handling of bugs with varying complexities. Experiments on extensive benchmarks demonstrate that FixAgent significantly outperforms state-of-the-art repair methods, fixing 1.25\$\textbackslash times\$ to 2.56\$\textbackslash times\$ bugs on the repo-level benchmark, Defects4J. This performance is achieved without requiring ground-truth root-cause code statements, unlike the baselines. Our source code is available on https://github.com/AcceptePapier/UniDebugger.},
  pubstate = {prepublished}
}

@inproceedings{liImperceptibleTextSteganography2024,
  title = {Imperceptible {{Text Steganography}} Based on {{Group Chat}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Li, Fanxiao and Wei, Ping and Fu, Tingchao and Lin, Yu and Zhou, Wei},
  date = {2024-07},
  pages = {1--6},
  issn = {1945-788X},
  doi = {10.1109/ICME57554.2024.10687958},
  url = {https://ieeexplore.ieee.org/abstract/document/10687958},
  urldate = {2024-12-07},
  abstract = {Text steganography is a technique for hiding secret messages within texts. Previous approaches neglect the contextual relevance of generated stego texts (texts containing secrets) and consistently transmitted secret messages unidirectionally. This behavior is considered anomalous and thus arouses the suspicion of potential attackers. In this paper, we first propose a text steganography framework grounded in the group chat scenario named GCStego, aimed at enhancing the behavior imperceptibility. Additionally, we employ a large language model (LLM) to generate stego texts according to the chatting history and thus boosts the contextual relevance. The proposed scheme is well-suited for secret transmission in group chatting, where multiple agents can pass secret messages through stego texts like regular conversations. Furthermore, we propose token index-based encoding, position filtering and sentence split strategies to deliver the performance. Experimental results demonstrate the superiority of our proposed framework in terms of text semantic controllability, behavioral imperceptibility, and anti-steganalysis ability.},
  eventtitle = {2024 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})}
}

@online{liTransformerLiteHighefficiencyDeployment2024,
  title = {Transformer-{{Lite}}: {{High-efficiency Deployment}} of {{Large Language Models}} on {{Mobile Phone GPUs}}},
  shorttitle = {Transformer-{{Lite}}},
  author = {Li, Luchang and Qian, Sheng and Lu, Jie and Yuan, Lunxi and Wang, Rui and Xie, Qin},
  date = {2024-07-05},
  eprint = {2403.20041},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.20041},
  url = {http://arxiv.org/abs/2403.20041},
  urldate = {2025-03-12},
  abstract = {The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance using LLMs with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2\textasciitilde 3x speedup for the decoding speed.},
  pubstate = {prepublished},
  version = {3}
}

@inproceedings{lowDocumentMarkingIdentification1995,
  title = {Document Marking and Identification Using Both Line and Word Shifting},
  booktitle = {Proceedings of {{INFOCOM}}'95},
  author = {Low, S.H. and Maxemchuk, N.F. and Brassil, J.T. and O'Gorman, L.},
  date = {1995-04},
  volume = {2},
  pages = {853-860 vol.2},
  issn = {0743-166X},
  doi = {10.1109/INFCOM.1995.515956},
  url = {https://ieeexplore.ieee.org/abstract/document/515956},
  urldate = {2025-03-23},
  abstract = {Continues a study of document marking to deter illicit dissemination. An experiment performed reveals that the distortion on the photocopy of a document is very different in the vertical and horizontal directions. This leads to the strategy that marks a text line both vertically using line shifting and horizontally using word shifting. A line that is marked is always accompanied by two unmarked control lines one above and one below. They are used to measure distortions in the vertical and horizontal directions in order to decide whether line or word shift should be detected. Line shifts are detected using a centroid method that bases its decision on the relative distance of line centroids. Word shifts are detected using a correlation method that treats a profile as a waveform and decides whether it originated from a waveform whose middle block has been shifted left or right. The maximum likelihood detectors for both methods are given.},
  eventtitle = {Proceedings of {{INFOCOM}}'95}
}

@inproceedings{luoTextSteganographyHigh2017,
  title = {Text {{Steganography}} with {{High Embedding Rate}}: {{Using Recurrent Neural Networks}} to {{Generate Chinese Classic Poetry}}},
  shorttitle = {Text {{Steganography}} with {{High Embedding Rate}}},
  booktitle = {Proceedings of the 5th {{ACM Workshop}} on {{Information Hiding}} and {{Multimedia Security}}},
  author = {Luo, Yubo and Huang, Yongfeng},
  date = {2017-06-20},
  series = {{{IH}}\&amp;{{MMSec}} '17},
  pages = {99--104},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3082031.3083240},
  url = {https://dl.acm.org/doi/10.1145/3082031.3083240},
  urldate = {2025-03-12},
  abstract = {We propose a novel text steganography method using RNN Encoder-Decoder structure to generate quatrains, one genre of Chinese poetry. Compared to other text-generation based steganography methods which have either very low embedding rate or flaws in the naturalness of generated texts, our method has higher embedding rate and better text quality. In this paper, we use the LSTM Encoder-Decoder model to generate the first line of a quatrain with a keyword and then generate the following lines one by one. RNN has proved effective in generating poetry, but when applied to steganograpy, poetry quality decreases sharply, because of the redundancy we create to hide information. To overcome this problem, we propose a template-constrained generation method and develop a word-choosing approach using inner-word mutual information. Through a series of experiments, it is proven that our approach outperforms other poetry steganography methods in both embedding rate and poetry quality.},
  isbn = {978-1-4503-5061-7}
}

@online{luSmallLanguageModels2024,
  title = {Small {{Language Models}}: {{Survey}}, {{Measurements}}, and {{Insights}}},
  shorttitle = {Small {{Language Models}}},
  author = {Lu, Zhenyan and Li, Xiang and Cai, Dongqi and Yi, Rongjie and Liu, Fangming and Zhang, Xiwen and Lane, Nicholas D. and Xu, Mengwei},
  date = {2024-09-24},
  eprint = {2409.15790},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.15790},
  url = {http://arxiv.org/abs/2409.15790},
  urldate = {2025-03-11},
  abstract = {Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters, we survey 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, in-context learning, mathematics, and coding. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.},
  pubstate = {prepublished},
  version = {1}
}

@article{malikHighCapacityText2017,
  title = {A High Capacity Text Steganography Scheme Based on Huffman Compression and Color Coding},
  author = {Malik, Aruna and Sikka, Geeta and Verma, Harsh K.},
  date = {2017-07-04},
  journaltitle = {Journal of Information and Optimization Sciences},
  volume = {38},
  number = {5},
  pages = {647--664},
  publisher = {Taylor \& Francis},
  issn = {0252-2667},
  doi = {10.1080/02522667.2016.1197572},
  url = {https://doi.org/10.1080/02522667.2016.1197572},
  urldate = {2025-03-12},
  abstract = {In this paper, we propose a high capacity text steganography scheme by using a combination of Move to Front (MTF) encoding, huffman compression, and color coding. The forward email platform is used to hide the secret data. In this scheme, we first encode the secret data using MTF encoding to increase the similarity among the element of the secret data and then apply huffman compression technique on the resultant secret data to obtain huffman codes for condensing the size of the secret data. Part of the compressed secret data is embedded into the email addresses, and residual part of the secret data stream is hide in the text message using a user defined color coding table. To make optimal utilization of number of characters in email ids, the characters added to the email id to indicate the secret data bits are taken from the processed secret data. The new characters are appended just before the ‘@’ symbol of email ids. Hence, the hiding capacity is further increased. Experimental results show that our method performs much better than the existing methods in terms of hiding capacity.}
}

@online{mallisTechniquesKVCache2024,
  title = {Techniques for {{KV Cache Optimization}} in {{Large Language Models}}},
  author = {Mallis, Omri},
  date = {2024-02-25},
  url = {https://www.omrimallis.com/posts/techniques-for-kv-cache-optimization/},
  urldate = {2025-01-08},
  abstract = {This post explores techniques for optimizing the Key-Value (KV) cache in large language models, from Grouped-query attention to PagedAttention and distributed cache management.},
  langid = {english}
}

@online{mallisUnderstandingHowLLM2023,
  title = {Understanding How {{LLM}} Inference Works with Llama.Cpp},
  author = {Mallis, Omri},
  date = {2023-11-11},
  url = {https://www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/},
  urldate = {2024-12-03},
  abstract = {In this post we will understand how large language models (LLMs) answer user prompts by exploring the source code of llama.cpp, a C++ implementation of LLaMA, covering subjects such as tokenization, embedding, self-attention and sampling.},
  langid = {english}
}

@online{martinezCombiningGenerativeArtificial2023,
  title = {Combining {{Generative Artificial Intelligence}} ({{AI}}) and the {{Internet}}: {{Heading}} towards {{Evolution}} or {{Degradation}}?},
  shorttitle = {Combining {{Generative Artificial Intelligence}} ({{AI}}) and the {{Internet}}},
  author = {Martínez, Gonzalo and Watson, Lauren and Reviriego, Pedro and Hernández, José Alberto and Juarez, Marc and Sarkar, Rik},
  date = {2023-02-17},
  eprint = {2303.01255},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.01255},
  url = {http://arxiv.org/abs/2303.01255},
  urldate = {2025-03-19},
  abstract = {In the span of a few months, generative Artificial Intelligence (AI) tools that can generate realistic images or text have taken the Internet by storm, making them one of the technologies with fastest adoption ever. Some of these generative AI tools such as DALL-E, MidJourney, or ChatGPT have gained wide public notoriety. Interestingly, these tools are possible because of the massive amount of data (text and images) available on the Internet. The tools are trained on massive data sets that are scraped from Internet sites. And now, these generative AI tools are creating massive amounts of new data that are being fed into the Internet. Therefore, future versions of generative AI tools will be trained with Internet data that is a mix of original and AI-generated data. As time goes on, a mixture of original data and data generated by different versions of AI tools will populate the Internet. This raises a few intriguing questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve with the new data sets or degenerate? Will evolution introduce biases in subsequent generations of generative AI tools? In this document, we explore these questions and report some very initial simulation results using a simple image-generation AI tool. These results suggest that the quality of the generated images degrades as more AI-generated data is used for training thus suggesting that generative AI may degenerate. Although these results are preliminary and cannot be generalised without further study, they serve to illustrate the potential issues of the interaction between generative AI and the Internet.},
  pubstate = {prepublished}
}

@online{martinezUnderstandingInterplayGenerative2023,
  title = {Towards {{Understanding}} the {{Interplay}} of {{Generative Artificial Intelligence}} and the {{Internet}}},
  author = {Martínez, Gonzalo and Watson, Lauren and Reviriego, Pedro and Hernández, José Alberto and Juarez, Marc and Sarkar, Rik},
  date = {2023-06-08},
  eprint = {2306.06130},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.06130},
  url = {http://arxiv.org/abs/2306.06130},
  urldate = {2025-03-19},
  abstract = {The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or reduce diversity in subsequent generations of generative AI tools? What are the societal implications of the possible degradation of these models? Can we mitigate the effects of this feedback loop? In this document, we explore the effect of this interaction and report some initial results using simple diffusion models trained with various image datasets. Our results show that the quality and diversity of the generated images can degrade over time suggesting that incorporating AI-created data can have undesired effects on future versions of generative models.},
  pubstate = {prepublished}
}

@online{mathewHiddenPlainText2024,
  title = {Hidden in {{Plain Text}}: {{Emergence}} \& {{Mitigation}} of {{Steganographic Collusion}} in {{LLMs}}},
  shorttitle = {Hidden in {{Plain Text}}},
  author = {Mathew, Yohan and Matthews, Ollie and McCarthy, Robert and Velja, Joan and family=Witt, given=Christian Schroeder, prefix=de, useprefix=false and Cope, Dylan and Schoots, Nandi},
  date = {2024-10-02},
  eprint = {2410.03768},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.03768},
  url = {http://arxiv.org/abs/2410.03768},
  urldate = {2025-03-12},
  abstract = {The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render collusion practically undetectable. This underscores the need for evaluation frameworks to monitor and mitigate steganographic collusion capabilities. We address a crucial gap in the literature by demonstrating, for the first time, that robust steganographic collusion in LLMs can arise indirectly from optimization pressure. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography. Importantly, we find that emergent steganographic collusion can be robust to both passive steganalytic oversight of model outputs and active mitigation through communication paraphrasing. We contribute a novel model evaluation framework and discuss limitations and future work. Our findings imply that effective risk mitigation from steganographic collusion post-deployment requires innovation in passive and active oversight techniques.},
  pubstate = {prepublished},
  version = {1}
}

@article{mccullaghBinLadenSteganography2001,
  entrysubtype = {magazine},
  title = {Bin {{Laden}}: {{Steganography Master}}?},
  shorttitle = {Bin {{Laden}}},
  author = {McCullagh, Declan},
  date = {2001-02-07},
  journaltitle = {Wired},
  issn = {1059-1028},
  url = {https://www.wired.com/2001/02/bin-laden-steganography-master/},
  urldate = {2025-01-09},
  abstract = {Are the FBI and CIA using reports that Osama bin Laden and others are using messaging scrambling techniques to justify further restrictions of encryption and steganography programs? Declan McCullagh reports from Washington.},
  langid = {american}
}

@software{metaMetallamaLlamamodels2025,
  title = {Meta-Llama/Llama-Models},
  author = {{Meta}},
  date = {2025-03-14T12:52:56Z},
  origdate = {2024-06-27T22:14:09Z},
  url = {https://github.com/meta-llama/llama-models},
  urldate = {2025-03-14},
  abstract = {Utilities intended for use with Llama models.},
  organization = {Meta Llama}
}

@article{michaelsonJournalistsMore11002025,
  entrysubtype = {newspaper},
  title = {Journalists among More than 1,100 Arrested in {{Turkey}} Crackdown},
  author = {Michaelson, Ruth},
  date = {2025-03-24T17:04:13},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/world/2025/mar/24/journalists-among-more-than-1100-arrested-in-turkey-crackdown-istanbul},
  urldate = {2025-03-24},
  abstract = {Authorities ask X to block accounts as tens of thousands take to streets in largest anti-government protests in years},
  journalsubtitle = {World news},
  langid = {british}
}

@software{mozillaMozillaOchoLlamafile2025,
  title = {Mozilla-{{Ocho}}/Llamafile},
  author = {{Mozilla}},
  date = {2025-03-11T19:50:12Z},
  origdate = {2023-09-10T21:12:32Z},
  url = {https://github.com/Mozilla-Ocho/llamafile},
  urldate = {2025-03-11},
  abstract = {Distribute and run LLMs with a single file.},
  organization = {Mozilla Ocho}
}

@inproceedings{mukherjeeChatGPTBasedImage2023,
  title = {{{ChatGPT Based Image Steganography}} ({{CGIS}}): {{A Novel Intelligent Information Hiding Approach}} to {{Achieve Secure Covert Communication}}},
  shorttitle = {{{ChatGPT Based Image Steganography}} ({{CGIS}})},
  booktitle = {2023 {{First International Conference}} on {{Advances}} in {{Electrical}}, {{Electronics}} and {{Computational Intelligence}} ({{ICAEECI}})},
  author = {Mukherjee, Subhadip and Mukhopadhyay, Somnath and Sarkar, Sunita},
  date = {2023-10},
  pages = {1--5},
  doi = {10.1109/ICAEECI58247.2023.10370937},
  url = {https://ieeexplore.ieee.org/abstract/document/10370937},
  urldate = {2025-03-12},
  abstract = {Covert communication refers to the practice of exchanging information or messages in a discreet or secretive manner, with the intent of keeping the communication hidden from unintended or unauthorized recipients. Steganography is a widely used, strategy for establishing covert communication, for maintaining the privacy and security of sensitive information. Steganography ensures that the message remains concealed from surveillance or eavesdropping. Artificial Intelligence (AI) plays a crucial role in enhancing security across various domains of covert communication. ChatGPT is an AI model, specifically a natural language processing (NLP) model. It belongs to the broader category of AI models which can comprehend and produce language for humans. In this paper, a new image steganographic model named CGIS is developed for covert communication using ChatGPT. Proposed steganographic method has achieved 3.0 bpp of hiding capacity. In situations where confidentiality is critical, such as in military operations, intelligence agencies, or corporate strategies, proposed method ensures that the message cannot be detected and or extracted by any surveillance or eavesdropper.},
  eventtitle = {2023 {{First International Conference}} on {{Advances}} in {{Electrical}}, {{Electronics}} and {{Computational Intelligence}} ({{ICAEECI}})}
}

@online{obrienNebraskaTeenMother2022,
  title = {Nebraska Teen and Mother Facing Charges in Abortion-Related Case That Involved Obtaining Their {{Facebook}} Messages | {{CNN Business}}},
  author = {O'Brien, Sara and Duffy, Clare},
  date = {2022-08-10T15:00:43Z},
  url = {https://www.cnn.com/2022/08/10/tech/teen-charged-abortion-facebook-messages/index.html},
  urldate = {2024-11-26},
  abstract = {A Nebraska mother and her 18-year-old daughter are facing multiple charges in a case that involved police obtaining Facebook messages between the two that authorities allege show evidence of an illegal self-managed medication abortion, as well as a plan to hide the remains.},
  langid = {english},
  organization = {CNN}
}

@software{onnxruntimedevelopersONNXRuntime2018,
  title = {{{ONNX Runtime}}},
  author = {{ONNX Runtime developers}},
  date = {2018-11},
  origdate = {2018-11-10T02:22:53Z},
  url = {https://github.com/microsoft/onnxruntime},
  urldate = {2025-03-11},
  abstract = {ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator}
}

@online{onnxruntimeONNXRuntimeHome,
  title = {{{ONNX Runtime}} | {{Home}}},
  author = {{ONNX Runtime}},
  url = {https://onnxruntime.ai/},
  urldate = {2025-03-11},
  abstract = {Cross-platform accelerated machine learning. Built-in optimizations speed up training and inferencing with your existing technology stack.},
  langid = {english}
}

@software{openaiOpenaiTiktoken2025,
  title = {Openai/Tiktoken},
  author = {{OpenAI}},
  date = {2025-03-14T18:27:50Z},
  origdate = {2022-12-01T23:22:11Z},
  url = {https://github.com/openai/tiktoken},
  urldate = {2025-03-14},
  abstract = {tiktoken is a fast BPE tokeniser for use with OpenAI's models.},
  organization = {OpenAI}
}

@online{oversecOVERSECPrivacyAll2016,
  title = {{{OVERSEC}} - {{Privacy}} for All {{Android Apps}}},
  author = {{Oversec}},
  date = {2016},
  url = {https://www.oversec.io/},
  urldate = {2025-03-11}
}

@software{panchalShubham0204SmolChatAndroid2025,
  title = {Shubham0204/{{SmolChat-Android}}},
  author = {Panchal, Shubham},
  date = {2025-03-11T02:44:46Z},
  origdate = {2024-11-10T08:26:09Z},
  url = {https://github.com/shubham0204/SmolChat-Android},
  urldate = {2025-03-11},
  abstract = {Running any GGUF SLMs/LLMs locally, on-device in Android}
}

@article{panQuantumManybodyPhysics2025,
  title = {Quantum Many-Body Physics Calculations with Large Language Models},
  author = {Pan, Haining and Mudur, Nayantara and Taranto, William and Tikhanovskaya, Maria and Venugopalan, Subhashini and Bahri, Yasaman and Brenner, Michael P. and Kim, Eun-Ah},
  date = {2025-01-31},
  journaltitle = {Communications Physics},
  shortjournal = {Commun Phys},
  volume = {8},
  number = {1},
  pages = {1--8},
  publisher = {Nature Publishing Group},
  issn = {2399-3650},
  doi = {10.1038/s42005-025-01956-y},
  url = {https://www.nature.com/articles/s42005-025-01956-y},
  urldate = {2025-03-24},
  abstract = {Large language models (LLMs) have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly-used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate GPT-4’s performance in executing the calculation for 15 papers from the past decade, demonstrating that, with the correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases. Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps. We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases.},
  langid = {english}
}

@online{perezAppleOpposesJudges2016,
  title = {Apple Opposes Judge’s Order to Hack {{San Bernardino}} Shooter’s {{iPhone}}},
  author = {Perez, Evan and Hume, Tim},
  date = {2016-02-17T02:33:06Z},
  url = {https://www.cnn.com/2016/02/16/us/san-bernardino-shooter-phone-apple/index.html},
  urldate = {2025-03-18},
  abstract = {Apple is opposing a judge’s order to help the FBI break into the iPhone of one of the San Bernardino, California, shooters.},
  langid = {english},
  organization = {CNN}
}

@online{petitcolasInformationHidingHomepage,
  title = {The Information Hiding Homepage},
  author = {Petitcolas, Fabien A. P.},
  url = {http://www.petitcolas.net/steganography/},
  urldate = {2025-03-13},
  abstract = {The information hiding homepage},
  organization = {The information hiding homepage}
}

@article{petitcolasInformationHidingSurvey1999,
  title = {Information {{Hiding}} - {{A Survey}}},
  author = {Petitcolas, F.A.P. and Anderson, R.J. and Kuhn, M.G.},
  date = {1999-07},
  journaltitle = {Proceedings of the IEEE},
  volume = {87},
  number = {7},
  pages = {1062--1078},
  issn = {1558-2256},
  doi = {10.1109/5.771065},
  url = {https://ieeexplore.ieee.org/abstract/document/771065},
  urldate = {2025-03-23},
  abstract = {Information-hiding techniques have recently become important in a number of application areas. Digital audio, video, and pictures are increasingly furnished with distinguishing but imperceptible marks, which may contain a hidden copyright notice or serial number or even help to prevent unauthorized copying directly. Military communications systems make increasing use of traffic security techniques which, rather than merely concealing the content of a message using encryption, seek to conceal its sender, its receiver, or its very existence. Similar techniques are used in some mobile phone systems and schemes proposed for digital elections. Criminals try to use whatever traffic security properties are provided intentionally or otherwise in the available communications systems, and police forces try to restrict their use. However, many of the techniques proposed in this young and rapidly evolving field can trace their history back to antiquity, and many of them are surprisingly easy to circumvent. In this article, we try to give an overview of the field, of what we know, what works, what does not, and what are the interesting topics for research.},
  eventtitle = {Proceedings of the {{IEEE}}}
}

@online{petroniLanguageModelsKnowledge2019,
  title = {Language {{Models}} as {{Knowledge Bases}}?},
  author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
  date = {2019-09-04},
  eprint = {1909.01066},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1909.01066},
  url = {http://arxiv.org/abs/1909.01066},
  urldate = {2025-03-19},
  abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
  pubstate = {prepublished}
}

@online{pytorchPyTorch,
  title = {{{PyTorch}}},
  author = {{PyTorch}},
  url = {https://pytorch.org/},
  urldate = {2025-03-17},
  langid = {english},
  organization = {PyTorch}
}

@software{pytorchPytorchAndroiddemoapp2025,
  title = {Pytorch/Android-Demo-App},
  author = {{PyTorch}},
  date = {2025-03-16T08:10:20Z},
  origdate = {2019-09-27T03:11:18Z},
  url = {https://github.com/pytorch/android-demo-app},
  urldate = {2025-03-17},
  abstract = {PyTorch android examples of usage in applications},
  organization = {pytorch}
}

@software{pytorchPytorchExecutorch2025,
  title = {Pytorch/Executorch},
  author = {{PyTorch}},
  date = {2025-03-12T08:13:36Z},
  origdate = {2022-02-25T17:58:31Z},
  url = {https://github.com/pytorch/executorch},
  urldate = {2025-03-12},
  abstract = {On-device AI across mobile, embedded and edge for PyTorch},
  organization = {pytorch}
}

@article{rissanenArithmeticCoding1979,
  title = {Arithmetic {{Coding}}},
  author = {Rissanen, J. and Langdon, G. G.},
  date = {1979-03},
  journaltitle = {IBM Journal of Research and Development},
  volume = {23},
  number = {2},
  pages = {149--162},
  issn = {0018-8646},
  doi = {10.1147/rd.232.0149},
  url = {https://ieeexplore.ieee.org/abstract/document/5390830},
  urldate = {2024-12-15},
  abstract = {The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases. An outstanding feature of this technique is that alphabet extensions are not required. A complete decodability analysis is given. The relationship of arithmetic coding to other known nonblock codes is illuminated.},
  eventtitle = {{{IBM Journal}} of {{Research}} and {{Development}}}
}

@online{robertsonDocumentsRevealQaedas2012,
  title = {Documents Reveal al {{Qaeda}}’s Plans for Seizing Cruise Ships, Carnage in {{Europe}}},
  author = {Robertson, Nic and Cruickshank, Paul and Lister, Tim},
  date = {2012-04-30T19:08:07Z},
  url = {https://www.cnn.com/2012/04/30/world/al-qaeda-documents-future/index.html},
  urldate = {2025-02-18},
  abstract = {CNN has obtained access to details of al Qaeda documents which shed light on future plans including seizing cruise ships and causing carnage in Europe.},
  langid = {english},
  organization = {CNN}
}

@article{rubinArithmeticStreamCoding1979,
  title = {Arithmetic Stream Coding Using Fixed Precision Registers},
  author = {Rubin, F.},
  date = {1979-11},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {25},
  number = {6},
  pages = {672--675},
  issn = {1557-9654},
  doi = {10.1109/TIT.1979.1056107},
  url = {https://ieeexplore.ieee.org/abstract/document/1056107},
  urldate = {2025-03-14},
  abstract = {Algorithms are presented for encoding and decoding strings of characters as real binary fractions, using registers of fixed precision. The encoding is left to right and does not require blocking. The algorithms have storage requirementsO(N)and computation timeO(n łog\_2N)for string lengthnand alphabet sizeN.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}}
}

@inproceedings{salleeModelBasedSteganography2004,
  title = {Model-{{Based Steganography}}},
  booktitle = {Digital {{Watermarking}}},
  author = {Sallee, Phil},
  editor = {Kalker, Ton and Cox, Ingemar and Ro, Yong Man},
  date = {2004},
  pages = {154--167},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24624-4_12},
  abstract = {This paper presents an information-theoretic method for performing steganography and steganalysis using a statistical model of the cover medium. The methodology is general, and can be applied to virtually any type of media. It provides answers for some fundamental questions which have not been fully addressed by previous steganographic methods, such as how large a message can be hidden without risking detection by certain statistical methods, and how to achieve this maximum capacity. Current steganographic methods have been shown to be insecure against fairly simple statistical attacks. Using the model-based methodology, an example steganography method is proposed for JPEG images which achieves a higher embedding efficiency and message capacity than previous methods while remaining secure against first order statistical attacks.},
  isbn = {978-3-540-24624-4},
  langid = {english}
}

@online{schmidgallAgentLaboratoryUsing2025,
  title = {Agent {{Laboratory}}: {{Using LLM Agents}} as {{Research Assistants}}},
  shorttitle = {Agent {{Laboratory}}},
  author = {Schmidgall, Samuel and Su, Yusheng and Wang, Ze and Sun, Ximeng and Wu, Jialian and Yu, Xiaodong and Liu, Jiang and Liu, Zicheng and Barsoum, Emad},
  date = {2025-01-08},
  eprint = {2501.04227},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.04227},
  url = {http://arxiv.org/abs/2501.04227},
  urldate = {2025-03-24},
  abstract = {Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84\% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.},
  pubstate = {prepublished}
}

@online{schneierTerroristsScienceHiding2001,
  title = {Terrorists and the Science of Hiding Messages},
  author = {Schneier, Bruce},
  date = {2001-09-25},
  url = {https://www.zdnet.com/article/terrorists-and-the-science-of-hiding-messages/},
  urldate = {2025-01-09},
  abstract = {Security expert Bruce Schneier writes that terrorist groups may be using steganography to communicate, allowing communication without any group knowing the identity of the other.},
  langid = {english}
}

@online{schneierTerroristsSteganography2001,
  title = {Terrorists and Steganography},
  author = {Schneier, Bruce},
  date = {2001-09-23},
  url = {https://www.zdnet.com/article/terrorists-and-steganography/},
  urldate = {2025-02-18},
  abstract = {Security expert Bruce Schneier writes that terrorist groups may be using steganography to communicate, allowing communication without any group knowing the identity of the other.},
  langid = {english},
  organization = {ZDNET}
}

@online{sennrichNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  date = {2016-06-10},
  eprint = {1508.07909},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.07909},
  url = {http://arxiv.org/abs/1508.07909},
  urldate = {2025-03-12},
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  pubstate = {prepublished}
}

@article{shannonCommunicationTheorySecrecy1949,
  title = {Communication Theory of Secrecy Systems},
  author = {Shannon, C. E.},
  date = {1949-10},
  journaltitle = {The Bell System Technical Journal},
  volume = {28},
  number = {4},
  pages = {656--715},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1949.tb00928.x},
  url = {https://ieeexplore.ieee.org/document/6769090},
  urldate = {2025-03-13},
  abstract = {THE problems of cryptography and secrecy systems furnish an interesting application of communication theory.1 In this paper a theory of secrecy systems is developed. The approach is on a theoretical level and is intended to complement the treatment found in standard works on cryptography.2 There, a detailed study is made of the many standard types of codes and ciphers, and of the ways of breaking them. We will be more concerned with the general mathematical structure and properties of secrecy systems.},
  eventtitle = {The {{Bell System Technical Journal}}}
}

@online{shiCodeCorrectnessClosing2024,
  title = {From {{Code}} to {{Correctness}}: {{Closing}} the {{Last Mile}} of {{Code Generation}} with {{Hierarchical Debugging}}},
  shorttitle = {From {{Code}} to {{Correctness}}},
  author = {Shi, Yuling and Wang, Songsong and Wan, Chengcheng and Gu, Xiaodong},
  date = {2024-10-05},
  eprint = {2410.01215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.01215},
  url = {http://arxiv.org/abs/2410.01215},
  urldate = {2025-03-24},
  abstract = {While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9\% improvement in accuracy over seed generations in HumanEval and a 97.6\% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.},
  pubstate = {prepublished}
}

@inproceedings{shirali-shahrezaNewApproachPersian2006,
  title = {A {{New Approach}} to {{Persian}}/{{Arabic Text Steganography}}},
  booktitle = {5th {{IEEE}}/{{ACIS International Conference}} on {{Computer}} and {{Information Science}} and 1st {{IEEE}}/{{ACIS International Workshop}} on {{Component-Based Software Engineering}},{{Software Architecture}} and {{Reuse}} ({{ICIS-COMSAR}}'06)},
  author = {Shirali-Shahreza, M.H. and Shirali-Shahreza, M.},
  date = {2006-07},
  pages = {310--315},
  doi = {10.1109/ICIS-COMSAR.2006.10},
  url = {https://ieeexplore.ieee.org/abstract/document/1652009},
  urldate = {2025-03-23},
  abstract = {Conveying information secretly and establishing hidden relationship has been of interest since long past. Text documents have been widely used since very long time ago. Therefore, we have witnessed different method of hiding information in texts (text steganography) since past to the present. In this paper we introduce a new approach for steganography in Persian and Arabic texts. Considering the existence of too many points in Persian and Arabic phrases, in this approach, by vertical displacement of the points, we hide information in the texts. This approach can be categorized under feature coding methods. This method can be used for Persian/Arabic watermarking. Our method has been implemented by Java programming language},
  eventtitle = {5th {{IEEE}}/{{ACIS International Conference}} on {{Computer}} and {{Information Science}} and 1st {{IEEE}}/{{ACIS International Workshop}} on {{Component-Based Software Engineering}},{{Software Architecture}} and {{Reuse}} ({{ICIS-COMSAR}}'06)}
}

@inproceedings{shirali-shahrezaTextSteganographySMS2007,
  title = {Text {{Steganography}} in {{SMS}}},
  booktitle = {2007 {{International Conference}} on {{Convergence Information Technology}} ({{ICCIT}} 2007)},
  author = {Shirali-Shahreza, Mohammad and Shirali-Shahreza, M. Hassan},
  date = {2007-11},
  pages = {2260--2265},
  doi = {10.1109/ICCIT.2007.100},
  url = {https://ieeexplore.ieee.org/abstract/document/4420590},
  urldate = {2025-03-08},
  abstract = {One of the services used in mobile phone is the short message service (SMS) which is widely used by the public in all parts of the world especially in Asia and Europe. This service enables people to write and exchange short messages via mobile phone. Due to the limited size of SMS, lack of a proper keyboard on the mobile phone and to improve the speed of typing, new abbreviations have been invented for different words and phrases which has lead to the invention of a new language called SMS-texting. One of the main issues in communication is information security and privacy. There are many methods for secret communication and many researchers are working on steganography. In steganography the data is hidden in a cover media such as picture or text. The present paper offers a new method for secret exchange of information through SMS by using and developing abbreviation text steganography with the use of the invented language of SMS-texting. This project has been implemented by J2ME (Java 2 Micro Edition) programming language and tested on a Nokia N71 mobile phone.},
  eventtitle = {2007 {{International Conference}} on {{Convergence Information Technology}} ({{ICCIT}} 2007)}
}

@online{shumailovCurseRecursionTraining2024,
  title = {The {{Curse}} of {{Recursion}}: {{Training}} on {{Generated Data Makes Models Forget}}},
  shorttitle = {The {{Curse}} of {{Recursion}}},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  date = {2024-04-14},
  eprint = {2305.17493},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.17493},
  url = {http://arxiv.org/abs/2305.17493},
  urldate = {2025-03-19},
  abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
  pubstate = {prepublished}
}

@online{songLLMFeynmanLeveragingLarge2025,
  title = {{{LLM-Feynman}}: {{Leveraging Large Language Models}} for {{Universal Scientific Formula}} and {{Theory Discovery}}},
  shorttitle = {{{LLM-Feynman}}},
  author = {Song, Zhilong and Ju, Minggang and Ren, Chunjin and Li, Qiang and Li, Chongyi and Zhou, Qionghua and Wang, Jinlan},
  date = {2025-03-09},
  eprint = {2503.06512},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  doi = {10.48550/arXiv.2503.06512},
  url = {http://arxiv.org/abs/2503.06512},
  urldate = {2025-03-24},
  abstract = {Distilling the underlying principles from data has long propelled scientific breakthroughs. However, conventional data-driven machine learning -- lacking deep, contextual domain knowledge -- tend to yield opaque or over-complex models that are challenging to interpret and generalize. Here, we present LLM-Feynman, a framework that leverages the embedded expertise of large language models (LLMs) with systematic optimization to distill concise, interpretable formula from data and domain knowledge. Our framework seamlessly integrates automated feature engineering, LLM-based symbolic regression augmented by self-evaluation and iterative refinement, and formula interpretation via Monte Carlo tree search. Ablation studies show that incorporating domain knowledge and self-evaluation yields more accurate formula at equivalent formula complexity than conventional symbolic regression. Validation on datasets from Feynman physics lectures confirms that LLM-Feynman can rediscover over 90\% real physical formulas. Moreover, when applied to four key materials science tasks -- from classifying the synthesizability of 2D and perovskite structures to predicting ionic conductivity in lithium solid-state electrolytes and GW bandgaps in 2D materials -- LLM-Feynman consistently yields interpretable formula with accuracy exceeding 90\% and R2 values above 0.8. By transcending mere data fitting through the integration of deep domain knowledge, LLM-Feynman establishes a new paradigm for the automated discovery of generalizable scientific formula and theory across disciplines.},
  pubstate = {prepublished}
}

@online{spammimicSpammimic2000,
  title = {Spammimic},
  author = {{Spammimic}},
  date = {2000},
  url = {https://www.spammimic.com/},
  urldate = {2025-03-11},
  abstract = {and you thought spam was useless},
  langid = {english}
}

@software{srivastavaBhrigu123Huffmancoding2025,
  title = {Bhrigu123/Huffman-Coding},
  author = {Srivastava, Bhrigu},
  date = {2025-03-09T06:50:15Z},
  origdate = {2017-01-18T13:17:33Z},
  url = {https://github.com/bhrigu123/huffman-coding},
  urldate = {2025-03-20},
  abstract = {Python Implementaion of Huffman Coding - compression and decompression}
}

@inproceedings{steinebachNaturalLanguageSteganography2024,
  title = {Natural {{Language Steganography}} by {{ChatGPT}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Availability}}, {{Reliability}} and {{Security}}},
  author = {Steinebach, Martin},
  date = {2024-07-30},
  series = {{{ARES}} '24},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3664476.3670930},
  url = {https://dl.acm.org/doi/10.1145/3664476.3670930},
  urldate = {2024-11-12},
  abstract = {Natural language steganography as well as natural language watermarking have been challenging because of the complexity and lack of noise in natural language. But with the advent of LLMs like ChatGPT, controlled synthesis of written language has become available. In this work, we show how ChatGPT can be utilized to generate synthetic texts of a given topic that act as stego covers for hidden messages.},
  isbn = {979-8-4007-1718-5}
}

@video{tedGlennGreenwaldWhy2014,
  entrysubtype = {video},
  title = {Glenn {{Greenwald}}: {{Why}} Privacy Matters},
  shorttitle = {Glenn {{Greenwald}}},
  editor = {{TED}},
  editortype = {director},
  date = {2014-10-10},
  url = {https://www.youtube.com/watch?v=pcSlowAhvUk},
  urldate = {2025-03-11}
}

@article{thabitComparativeAnalysisArabic2021,
  title = {A {{Comparative Analysis}} of {{Arabic Text Steganography}}},
  author = {Thabit, Reema and Udzir, Nur Izura and Yasin, Sharifah Md and Asmawi, Aziah and Roslan, Nuur Alifah and Din, Roshidi},
  date = {2021-01},
  journaltitle = {Applied Sciences},
  volume = {11},
  number = {15},
  pages = {6851},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app11156851},
  url = {https://www.mdpi.com/2076-3417/11/15/6851},
  urldate = {2025-03-12},
  abstract = {Protecting sensitive information transmitted via public channels is a significant issue faced by governments, militaries, organizations, and individuals. Steganography protects the secret information by concealing it in a transferred object such as video, audio, image, text, network, or DNA. As text uses low bandwidth, it is commonly used by Internet users in their daily activities, resulting a vast amount of text messages sent daily as social media posts and documents. Accordingly, text is the ideal object to be used in steganography, since hiding a secret message in a text makes it difficult for the attacker to detect the hidden message among the massive text content on the Internet. Language’s characteristics are utilized in text steganography. Despite the richness of the Arabic language in linguistic characteristics, only a few studies have been conducted in Arabic text steganography. To draw further attention to Arabic text steganography prospects, this paper reviews the classifications of these methods from its inception. For analysis, this paper presents a comprehensive study based on the key evaluation criteria (i.e., capacity, invisibility, robustness, and security). It opens new areas for further research based on the trends in this field.},
  issue = {15},
  langid = {english}
}

@online{tianDebugBenchEvaluatingDebugging2024,
  title = {{{DebugBench}}: {{Evaluating Debugging Capability}} of {{Large Language Models}}},
  shorttitle = {{{DebugBench}}},
  author = {Tian, Runchu and Ye, Yining and Qin, Yujia and Cong, Xin and Lin, Yankai and Pan, Yinxu and Wu, Yesai and Hui, Haotian and Liu, Weichuan and Liu, Zhiyuan and Sun, Maosong},
  date = {2024-06-06},
  eprint = {2401.04621},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.04621},
  url = {http://arxiv.org/abs/2401.04621},
  urldate = {2025-03-24},
  abstract = {Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.},
  pubstate = {prepublished}
}

@online{turnerIntroductionTransformers2024,
  title = {An {{Introduction}} to {{Transformers}}},
  author = {Turner, Richard E.},
  date = {2024-02-08},
  eprint = {2304.10557},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.10557},
  url = {http://arxiv.org/abs/2304.10557},
  urldate = {2025-03-12},
  abstract = {The transformer is a neural network component that can be used to learn useful representations of sequences or sets of data-points. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture. We will not discuss training as this is rather standard. We assume that the reader is familiar with fundamental topics in machine learning including multi-layer perceptrons, linear transformations, softmax functions and basic probability.},
  pubstate = {prepublished},
  version = {5}
}

@online{united4iranNahoft2021,
  title = {Nahoft},
  author = {{United 4 Iran}},
  date = {2021},
  url = {https://nahoftapp.com/index-en.html},
  urldate = {2025-03-11}
}

@software{united4iranU4iadminNahoft2025,
  title = {U4i-Admin/{{Nahoft}}},
  author = {{United 4 Iran}},
  date = {2025-01-09T21:35:47Z},
  origdate = {2021-07-22T16:06:15Z},
  url = {https://github.com/u4i-admin/Nahoft},
  urldate = {2025-03-24},
  organization = {U4I}
}

@software{vali-98Vali98ChatterUI2025,
  title = {Vali-98/{{ChatterUI}}},
  author = {{Vali-98}},
  date = {2025-03-11T17:42:50Z},
  origdate = {2023-10-18T11:28:06Z},
  url = {https://github.com/Vali-98/ChatterUI},
  urldate = {2025-03-11},
  abstract = {Simple frontend for LLMs built in react-native.}
}

@online{vandersarMetaTorrented812025,
  title = {'{{Meta Torrented}} over 81 {{TB}} of {{Data Through Anna}}'s {{Archive}}, {{Despite Few Seeders}}' * {{TorrentFreak}}},
  author = {Van der Sar, Ernesto},
  date = {2025-02-06},
  url = {https://torrentfreak.com/meta-torrented-over-81-tb-of-data-through-annas-archive-despite-few-seeders-250206/},
  urldate = {2025-03-19}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-12-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished}
}

@online{vendrowLargeLanguageModel2025,
  title = {Do {{Large Language Model Benchmarks Test Reliability}}?},
  author = {Vendrow, Joshua and Vendrow, Edward and Beery, Sara and Madry, Aleksander},
  date = {2025-02-05},
  eprint = {2502.03461},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.03461},
  url = {http://arxiv.org/abs/2502.03461},
  urldate = {2025-03-24},
  abstract = {When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also reliable. Many benchmarks have been created to track LLMs' growing capabilities, however there has been no similar focus on measuring their reliability. To understand the potential ramifications of this gap, we investigate how well current benchmarks quantify model reliability. We find that pervasive label errors can compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior. Motivated by this gap in the evaluation of reliability, we then propose the concept of so-called platinum benchmarks, i.e., benchmarks carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that, indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures further reveals previously unidentified patterns of problems on which frontier models consistently struggle. We provide code at https://github.com/MadryLab/platinum-benchmarks},
  pubstate = {prepublished}
}

@online{wangHistoryDevelopmentPrinciples2024,
  title = {History, {{Development}}, and {{Principles}} of {{Large Language Models-An Introductory Survey}}},
  author = {Wang, Zichong and Chu, Zhibo and Doan, Thang Viet and Ni, Shiwen and Yang, Min and Zhang, Wenbin},
  date = {2024-09-23},
  eprint = {2402.06853},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.06853},
  url = {http://arxiv.org/abs/2402.06853},
  urldate = {2025-03-19},
  abstract = {Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLM reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.},
  pubstate = {prepublished}
}

@article{watkinsSteganographyMessagesHidden2001,
  title = {Steganography – {{Messages Hidden}} in {{Bits}}},
  author = {Watkins, Jonathan},
  date = {2001},
  journaltitle = {Multimedia Systems Coursework, Dept of Electronics and CS, University of Southampton, SO17 1BJ, UK},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8c4c24eac97067ee16d8ecdc999cacb7f62c068c},
  abstract = {Steganography is the process of hiding one medium of communication (text, sound or image) within another. This paper will discuss the tools used to both hide and unhide (know as Steganalysis) information. A look at the history starting with Herodotus in ancient Greece describing secret messages written in wax on stone tablets, to world war two’s secret double meaning Nazi messages and British Intelligences invisible ink. Most recently the techniques have been accredited with Osama Bin Laden's Al-Qa’eda terrorist network. Not all of Steganography involves some kind of subterfuge, I will also cover the area of digital watermarking, a method to try to protect the copyright of image.},
  langid = {english}
}

@article{waynerMimicFunctions1992,
  title = {Mimic {{Functions}}},
  author = {Wayner, Peter},
  date = {1992-07-01},
  journaltitle = {Cryptologia},
  volume = {16},
  number = {3},
  pages = {193--214},
  publisher = {Taylor \& Francis},
  issn = {0161-1194},
  doi = {10.1080/0161-119291866883},
  url = {https://doi.org/10.1080/0161-119291866883},
  urldate = {2025-03-16},
  abstract = {A mimic function changes a file A so it assumes the statistical properties of another file B. That is, if p(t, A) is the probability of some substring t occurring in A, then a mimic function f, recodes A so that p(t, f(A)) approximates p(t, B) for all strings t of length less than some n. This paper describes the algorithm for computing mimic functions and compares the algorithm with its functional inverse, Huffman coding. The paper also provides a description of more robust and more general mimic functions which can be defined using context-free grammars and van Wijngaarden grammars.}
}

@article{weiJailbrokenHowDoes2023,
  title = {Jailbroken: {{How Does LLM Safety Training Fail}}?},
  shorttitle = {Jailbroken},
  author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {80079--80110},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html},
  urldate = {2025-03-12},
  langid = {english}
}

@article{wongSocialMediaMay2016,
  entrysubtype = {newspaper},
  title = {Social Media May Have Been Blocked during {{Turkey}} Coup Attempt},
  author = {Wong, Julia Carrie},
  date = {2016-07-15T23:25:09},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/world/2016/jul/15/turkey-blocking-social-facebook-twitter-youtube},
  urldate = {2025-03-24},
  abstract = {Reports emerge during attempted military coup of people struggling to access social media in a country described as a ‘bastion of internet censorship’},
  journalsubtitle = {World news},
  langid = {british}
}

@inproceedings{wuGenerativeTextSteganography2024,
  title = {Generative {{Text Steganography}} with {{Large Language Model}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Multimedia}}},
  author = {Wu, Jiaxuan and Wu, Zhengxian and Xue, Yiming and Wen, Juan and Peng, Wanli},
  date = {2024-10-28},
  eprint = {2404.10229},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {10345--10353},
  doi = {10.1145/3664647.3680562},
  url = {http://arxiv.org/abs/2404.10229},
  urldate = {2025-03-12},
  abstract = {Recent advances in large language models (LLMs) have blurred the boundary of high-quality text generation between humans and machines, which is favorable for generative text steganography. While, current advanced steganographic mapping is not suitable for LLMs since most users are restricted to accessing only the black-box API or user interface of the LLMs, thereby lacking access to the training vocabulary and its sampling probabilities. In this paper, we explore a black-box generative text steganographic method based on the user interfaces of large language models, which is called LLM-Stega. The main goal of LLM-Stega is that the secure covert communication between Alice (sender) and Bob (receiver) is conducted by using the user interfaces of LLMs. Specifically, We first construct a keyword set and design a new encrypted steganographic mapping to embed secret messages. Furthermore, to guarantee accurate extraction of secret messages and rich semantics of generated stego texts, an optimization mechanism based on reject sampling is proposed. Comprehensive experiments demonstrate that the proposed LLM-Stega outperforms current state-of-the-art methods.}
}

@article{wuPromptingSteganographyNew2024,
  title = {Prompting {{Steganography}}: {{A New Paradigm}}},
  shorttitle = {Prompting {{Steganography}}},
  author = {Wu, Hanzhou},
  date = {2024-01-21},
  journaltitle = {Electronic Imaging},
  volume = {36},
  pages = {1--11},
  publisher = {{Society for Imaging Science and Technology}},
  issn = {2470-1173},
  doi = {10.2352/EI.2024.36.4.MWSF-338},
  url = {https://library.imaging.org/ei/articles/36/4/MWSF-338},
  urldate = {2025-03-12},
  abstract = {Abstract Recent studies show that scaling pre-trained language models can lead to a significantly improved model capacity on downstream tasks, resulting in a new research direction called large language models (LLMs). A remarkable application of LLMs is ChatGPT, which is a powerful large language model capable of generating human-like text based on context and past conversations. It is demonstrated that LLMs have impressive skills in reasoning, especially when using prompting strategies. In this paper, we explore the possibility of applying LLMs to the field of steganography, which is referred to as the art of hiding secret data into an innocent cover for covert communication. Our purpose is not to combine an LLM into an already designed steganographic system to boost the performance, which follows the conventional framework of steganography. Instead, we expect that, through prompting, an LLM can realize steganography by itself, which is defined as prompting steganography and may be a new paradigm of steganography. We show that, by reasoning, an LLM can embed secret data into a cover, and extract secret data from a stego, with an error rate. This error rate, however, can be reduced by optimizing the prompt, which may shed light on further research.},
  langid = {english}
}

@article{xiangGenerativeLinguisticSteganography2022,
  title = {Generative {{Linguistic Steganography}}: {{A Comprehensive Review}}},
  shorttitle = {Generative {{Linguistic Steganography}}},
  author = {Xiang, Lingyun and Wang, Rong and Yang, Zhongliang and Liu, Yuling},
  date = {2022},
  journaltitle = {KSII Transactions on Internet and Information Systems (TIIS)},
  volume = {16},
  number = {3},
  pages = {986--1005},
  publisher = {Korean Society for Internet Information},
  issn = {1976-7277},
  doi = {10.3837/tiis.2022.03.013},
  url = {https://koreascience.kr/article/JAKO202211563864037.page},
  urldate = {2025-03-13},
  abstract = {Text steganography is one of the most imminent and promising research interests in the information security field. With the unprecedented success of the neural network and natural language processing (NLP), the last years have seen a surge of research on generative linguistic steganography (GLS). This paper provides a thorough and comprehensive review to summarize the existing key contributions, and creates a novel taxonomy for GLS according to NLP techniques and steganographic encoding algorithm, then summarizes the characteristics of generative linguistic steganographic methods properly to analyze the relationship and difference between each type of them. Meanwhile, this paper also comprehensively introduces and analyzes several evaluation metrics to evaluate the performance of GLS from diverse perspective. Finally, this paper concludes the future research work, which is more conducive to the follow-up research and innovation of researchers.},
  langid = {english}
}

@article{yangLinguisticSteganalysisSocial2023,
  title = {Linguistic {{Steganalysis Toward Social Network}}},
  author = {Yang, Jinshuai and Yang, Zhongliang and Zou, Jiajun and Tu, Haoqin and Huang, Yongfeng},
  date = {2023},
  journaltitle = {IEEE Transactions on Information Forensics and Security},
  volume = {18},
  pages = {859--871},
  issn = {1556-6021},
  doi = {10.1109/TIFS.2022.3226909},
  url = {https://ieeexplore.ieee.org/abstract/document/9970400},
  urldate = {2025-03-13},
  abstract = {With the rapid development of the internet and social media, linguistic steganography can be easily abused in social networks to make considerable damage to varied aspects like personal privacy, network virus and national defense. Currently, considerable linguistic steganalysis methods are proposed to detect harmful steganographic carriers. However, almost all the existing methods fail in real social networks, since they are only devoted to the linguistic features that are extreme insufficient owing to the extreme sparsity and extreme fragmentation challenges of real social networks. In this paper, we attempt to fill the long-standing gap that the datasets and effective methods are absent for hunting steganographic texts in social network scenarios. Concretely, we construct a dataset called Stego-Sandbox to simulate the real social network scenarios, which contains texts and their relation. And we propose an effective linguistic steganalysis framework integrating linguistic features contained in texts and context features represented by these connections. Extensive experimental results demonstrate owing to the captured context features, our proposed framework can effectively compensate for shortcomings of these existing methods and tremendously improve their detection ability in real social network scenarios.},
  eventtitle = {{{IEEE Transactions}} on {{Information Forensics}} and {{Security}}}
}

@article{yangRNNStegaLinguisticSteganography2019,
  title = {{{RNN-Stega}}: {{Linguistic Steganography Based}} on {{Recurrent Neural Networks}}},
  shorttitle = {{{RNN-Stega}}},
  author = {Yang, Zhong-Liang and Guo, Xiao-Qing and Chen, Zi-Ming and Huang, Yong-Feng and Zhang, Yu-Jin},
  date = {2019-05},
  journaltitle = {IEEE Transactions on Information Forensics and Security},
  volume = {14},
  number = {5},
  pages = {1280--1295},
  issn = {1556-6021},
  doi = {10.1109/TIFS.2018.2871746},
  url = {https://ieeexplore.ieee.org/document/8470163},
  urldate = {2025-01-26},
  abstract = {Linguistic steganography based on text carrier auto-generation technology is a current topic with great promise and challenges. Limited by the text automatic generation technology or the corresponding text coding methods, the quality of the steganographic text generated by previous methods is inferior, which makes its imperceptibility unsatisfactory. In this paper, we propose a linguistic steganography based on recurrent neural networks, which can automatically generate high-quality text covers on the basis of a secret bitstream that needs to be hidden. We trained our model with a large number of artificially generated samples and obtained a good estimate of the statistical language model. In the text generation process, we propose fixed-length coding and variable-length coding to encode words based on their conditional probability distribution. We designed several experiments to test the proposed model from the perspectives of information hiding efficiency, information imperceptibility, and information hidden capacity. The experimental results show that the proposed model outperforms all the previous related methods and achieves the state-of-the-art performance.},
  eventtitle = {{{IEEE Transactions}} on {{Information Forensics}} and {{Security}}}
}

@article{yangSeSyLinguisticSteganalysis2022,
  title = {{{SeSy}}: {{Linguistic Steganalysis Framework Integrating Semantic}} and {{Syntactic Features}}},
  shorttitle = {{{SeSy}}},
  author = {Yang, Jinshuai and Yang, Zhongliang and Zhang, Siyu and Tu, Haoqin and Huang, Yongfeng},
  date = {2022},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {29},
  pages = {31--35},
  issn = {1558-2361},
  doi = {10.1109/LSP.2021.3122901},
  url = {https://ieeexplore.ieee.org/abstract/document/9591452},
  urldate = {2025-03-13},
  abstract = {With the rapid development of natural language processing technology and linguistic steganography, linguistic steganalysis gains considerable interest in recent years. Current advanced methods dominantly focus on statistical features in semantic view yet ignore syntax structure of text, which leads to limited performance to some newly statistically indistinguishable steganography algorithms. To fill this gap, in this paper, we propose a novel linguistic steganalysis framework named SeSy to integrate both semantic and syntactic features. Specifically, we propose to employ transformer-architecture language model as semantics extractor and leverage a graph attention network to retain syntactic features. Extensive experimental results show that owing to additional syntactic information, the SeSy framework effectively brings about remarkable improvement to current advanced linguistic steganalysis methods.},
  eventtitle = {{{IEEE Signal Processing Letters}}}
}

@inproceedings{yiExploitingLanguageModel2022,
  title = {Exploiting {{Language Model For Efficient Linguistic Steganalysis}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yi, Biao and Wu, Hanzhou and Feng, Guorui and Zhang, Xinpeng},
  date = {2022-05},
  pages = {3074--3078},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746219},
  url = {https://ieeexplore.ieee.org/abstract/document/9746219},
  urldate = {2025-03-13},
  abstract = {Recent advances in linguistic steganalysis have successively applied CNN, RNN, GNN and other efficient deep models for detecting secret information in generative texts. These methods tend to seek stronger feature extractors to achieve higher steganalysis effects. However, we have found through experiments that there actually exists significant difference between automatically generated stego texts and carrier texts in terms of the conditional probability distribution of individual words. Such kind of difference can be naturally captured by the language model used for generating stego texts. Through further experiments, we conclude that this ability can be transplanted to a text classifier by pre-training and fine-tuning to improve the detection performance. Motivated by this insight, we propose two methods for efficient linguistic steganalysis. One is to pre-train a language model based on RNN, and the other is to pre-train a sequence autoencoder. The results indicate that the two methods have different degrees of performance gain compared to the randomly initialized RNN, and the convergence speed is significantly accelerated. Moreover, our methods achieved the best performance compared to related works, while providing a solution for real-world scenario where there are more cover texts than stego texts.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@online{zamirExcuseMeSir2024,
  title = {Excuse Me, Sir? {{Your}} Language Model Is Leaking (Information)},
  shorttitle = {Excuse Me, Sir?},
  author = {Zamir, Or},
  date = {2024-01-18},
  eprint = {2401.10360},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.10360},
  url = {http://arxiv.org/abs/2401.10360},
  urldate = {2025-03-12},
  abstract = {We introduce a cryptographic method to hide an arbitrary secret payload in the response of a Large Language Model (LLM). A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload. In particular, the quality of generated text is not affected by the payload. Our approach extends a recent result of Christ, Gunn and Zamir (2023) who introduced an undetectable watermarking scheme for LLMs.},
  pubstate = {prepublished},
  version = {1}
}

@online{zhaoLinguaLinkedDistributedLarge2023,
  title = {{{LinguaLinked}}: {{A Distributed Large Language Model Inference System}} for {{Mobile Devices}}},
  shorttitle = {{{LinguaLinked}}},
  author = {Zhao, Junchen and Song, Yurun and Liu, Simeng and Harris, Ian G. and Jyothi, Sangeetha Abdu},
  date = {2023-12-01},
  eprint = {2312.00388},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2312.00388},
  url = {http://arxiv.org/abs/2312.00388},
  urldate = {2024-11-12},
  abstract = {Deploying Large Language Models (LLMs) locally on mobile devices presents a significant challenge due to their extensive memory requirements. In this paper, we introduce LinguaLinked, a system for decentralized, distributed LLM inference on mobile devices. LinguaLinked enables collaborative execution of the inference task across multiple trusted devices. LinguaLinked ensures data privacy by processing information locally. LinguaLinked uses three key strategies. First, an optimized model assignment technique segments LLMs and uses linear optimization to align segments with each device's capabilities. Second, an optimized data transmission mechanism ensures efficient and structured data flow between model segments while also maintaining the integrity of the original model structure. Finally, LinguaLinked incorporates a runtime load balancer that actively monitors and redistributes tasks among mobile devices to prevent bottlenecks, enhancing the system's overall efficiency and responsiveness. We demonstrate that LinguaLinked facilitates efficient LLM inference while maintaining consistent throughput and minimal latency through extensive testing across various mobile devices, from high-end to low-end Android devices. In our evaluations, compared to the baseline, LinguaLinked achieves an inference performance acceleration of \$1.11\textbackslash times\$ to \$1.61\textbackslash times\$ in single-threaded settings, \$1.73\textbackslash times\$ to \$2.65\textbackslash times\$ with multi-threading. Additionally, runtime load balancing yields an overall inference acceleration of \$1.29\textbackslash times\$ to \$1.32\textbackslash times\$.},
  pubstate = {prepublished}
}

@software{zieglerHarvardnlpNeuralSteganography2025,
  title = {Harvardnlp/{{NeuralSteganography}}},
  author = {Ziegler, Zachary M.},
  date = {2025-01-17T12:54:40Z},
  origdate = {2019-08-30T05:38:12Z},
  url = {https://github.com/harvardnlp/NeuralSteganography},
  urldate = {2025-03-11},
  abstract = {STEGASURAS: STEGanography via Arithmetic coding and Strong neURAl modelS},
  organization = {HNLP}
}

@online{zieglerNeuralLinguisticSteganography2019,
  title = {Neural {{Linguistic Steganography}}},
  author = {Ziegler, Zachary M. and Deng, Yuntian and Rush, Alexander M.},
  date = {2019-09-03},
  eprint = {1909.01496},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1909.01496},
  url = {http://arxiv.org/abs/1909.01496},
  urldate = {2024-10-30},
  abstract = {Whereas traditional cryptography encrypts a secret message into an unintelligible form, steganography conceals that communication is taking place by encoding a secret message into a cover signal. Language is a particularly pragmatic cover signal due to its benign occurrence and independence from any one medium. Traditionally, linguistic steganography systems encode secret messages in existing text via synonym substitution or word order rearrangements. Advances in neural language models enable previously impractical generation-based techniques. We propose a steganography technique based on arithmetic coding with large-scale neural language models. We find that our approach can generate realistic looking cover sentences as evaluated by humans, while at the same time preserving security by matching the cover message distribution with the language model distribution.},
  pubstate = {prepublished},
  version = {1}
}

@online{zieglerStegasuras2025,
  title = {Stegasuras},
  author = {Ziegler, Zachary M. and Deng, Yuntian and Rush, Alexander M.},
  date = {2025-03-11},
  url = {https://steganography.live/},
  urldate = {2025-03-11},
  abstract = {STEGanography via Arithmetic coding and Strong neURAl modelS},
  organization = {Stegasuras}
}
